CURSO GOOGLE CLOUD CERTIFICAÇÃO.


>>> Video 1 <<<  5 minutos e 37

Olá, sou a Lak e lidero uma equipe que ajuda os clientes do Google Cloud a criar aplicativos que usam nossos produtos de Big Data e de aprendizado de máquina. Entre as coisas que fazemos é criar cursos e laboratórios de treinamento em big data e aprendizado de máquina; como este curso, os fundamentos de Big Data e Machine Learning com o Google Cloud Platform. Este curso foi criado para exibir dados reais e desafios de ML e oferecer a você experiência prática na solução desses desafios usando o Google Cloud. É um curso crítico para o mestre, pois abrange os casos de uso mais comuns que você e sua equipe encontrarão em sua jornada de big data. Este curso é dividido em seis módulos de conteúdo. No primeiro módulo, que é baseado na tomada de decisões baseada em dados, você aprenderá tudo sobre as ferramentas de dados e ML disponíveis no GCP, a partir de uma perspectiva organizacional de alto nível. Em seguida, nos próximos quatro módulos, você será apresentado aos produtos do Google Cloud no contexto em que eles são empregados para resolver problemas do mundo real. Nos quatro módulos, além de ampliar seu conhecimento sobre nossos produtos e plataforma, você também poderá praticar com o Qwiklabs prático. Por fim, no módulo de resumo, recapitularemos tudo o que você aprendeu neste curso e forneceremos recursos adicionais sobre os tópicos que você fez. Em cada módulo, esta é a ordem típica que seguiremos. Primeiro, teremos palestras de especialistas no assunto, onde apresentaremos o cenário do Big Data e os desafios associados e como abordá-los com as tecnologias do Cloud. Em seguida, você verá uma demonstração da solução e da ação que destacará os principais recursos que você aprenderá e praticará em seus laboratórios. Depois de entender os cenários e assistir às demonstrações, é hora de praticar com o Qwiklabs em uma conta real do Google Cloud Platform. Por fim, você explorará casos e arquiteturas reais de uso do cliente, para que você possa se familiarizar com as melhores práticas e se inspirar em suas próprias soluções. Em outras palavras, descrevemos uma classe comum de big data e problemas de ML e aprimoramos um problema específico e, em seguida, mostramos uma demonstração de uma solução para esse problema. Então falamos sobre por que construímos uma solução desse jeito, usando isso, oportunidade de cobrir como e quando usar os vários produtos usados ​​na solução. Finalmente, ampliamos a lente e mostramos a você aplicações do mundo real que são variantes dos princípios abordados no capítulo. Você já está fazendo este curso, o que significa que reconhece a importância do processamento de big data. Mas por que essa habilidade é tão exigida? De acordo com a pesquisa da McKinsey, até 2020, teremos 50 bilhões de dispositivos conectados à Internet das Coisas. Esses dispositivos farão com que o fornecimento de dados seja duplicado a cada dois anos. Infelizmente, apenas cerca de 1% dos dados gerados hoje são analisados ​​de acordo com a McKinsey. Esse estado de coisas oferece uma oportunidade aberta porque há muito valor nos dados. Acredito que a capacidade de criar aplicativos que manipulem grandes quantidades de dados e derivem insights desses dados de maneira automatizada. Essa habilidade é uma habilidade que será bem recompensada no mercado. Indivíduos que possuem essa habilidade terão muitas oportunidades abertas para eles e as empresas que desenvolverem essa habilidade terão mais sucesso. Então, a oportunidade para analistas de dados, cientistas de dados e engenheiros de dados nos falarão sobre quais são esses papéis e quais são as diferenças. A oportunidade para todos esses três papéis é muito clara. Em essência, este curso é voltado principalmente para engenheiros de dados. Dito isso, se você é um analista, um engenheiro de ML ou um líder de tecnologia para sua equipe, é uma habilidade valiosa saber como todos os produtos de Big Data e ML interagem para solucionar alguns dos desafios mais comuns enfrentados pelos engenheiros de dados. Quais são esses desafios? Esses desafios estão migrando as cargas de trabalho de big data existentes para um ambiente em que você possa analisar com eficácia todos os seus dados, analisando interativamente grandes e grandes, quero dizer terabytes a petabytes; análise de grandes conjuntos de dados de dados históricos. Terceiro, criar pipelines escalonáveis ​​que possam manipular dados de streaming, para que sua empresa possa tomar decisões baseadas em dados mais rapidamente. Por fim, ao criar modelos de aprendizado de máquina para que você não esteja apenas reagindo aos dados, é possível fazer ações preditivas voltadas para o futuro usando seus dados.


>>> Video 2 <<< 3 minutos e 40

Oi eu sou Lak. Bem-vindo ao primeiro módulo do nosso curso Fundamentos de Big Data. Ele fornece uma introdução ao Google Cloud Platform. Neste módulo, examinaremos a infraestrutura por trás do Google Cloud Platform ou do GCP, que foi originalmente criado para potencializar os próprios aplicativos do Google e agora está disponível para você. Depois, abordaremos os produtos de big data e ML que são construídos sobre essa infraestrutura e quando você deve escolher quais produtos para sua arquitetura de solução. Depois disso, minha parte favorita deste curso, aprender com outros clientes que usam o Google Cloud explorando seus casos de uso e se inspirando para resolver desafios semelhantes para suas próprias equipes e projetos. Você aprenderá onde pesquisar estudos de caso de referência de clientes do GCP por setor ou por produto e, em seguida, examinará a arquitetura da solução em uma atividade curta. Construir a estrutura de equipe correta é fundamental para solucionar esses desafios de big data. Vamos explorar os diferentes tipos de papéis e personas para construir uma equipe de big data de sucesso em sua organização. Considere, por um segundo, o impacto que a Pesquisa Google tem em nosso dia a dia com respostas oportunas e relevantes. Agora pense em outros produtos do Google: Gmail, Google Maps, Chrome, YouTube, Android, Play e Drive. Cada um desses produtos tem mais de um bilhão de usuários mensais. O Google precisou desenvolver a infraestrutura para ingerir, gerenciar e atender todos os dados desses aplicativos, e para isso, com uma base de usuários crescente e requisitos de dados que estão em constante evolução. Sete produtos com um bilhão de usuários. Na verdade, há um oitavo produto do Google que tem um bilhão de usuários finais, o Google Cloud, exceto que são seus usuários. Os usuários finais atendidos pelos clientes do Google Cloud, como o Home Depot, o Spotify, o Twitter, o New York Times, o Colgate-Palmolive e o Go Check. Vejamos os blocos de construção por trás da infraestrutura de big data do Google e como você pode aproveitá-la com o Google Cloud. Existem quatro aspectos fundamentais da infraestrutura básica do Google e uma camada superior de produtos e serviços com os quais você irá interagir com mais frequência. A camada de base que cobre todos os aplicativos do Google e, portanto, o Google Cloud também, é segurança. Além disso, são computação, armazenamento e rede. Elas permitem processar, armazenar e entregar insights, pipelines de dados e modelos de aprendizado de máquina que mudam os negócios. Por fim, ao executar seus aplicativos de big data em máquinas virtuais bare metal, o Google desenvolveu uma camada superior de big data e produtos ML para abstrair o trabalho pesado de gerenciar e dimensionar essa infraestrutura para você.


>>> Video 3 <<< 9 minutos 41 segundos.

Uma organização de dados crescente como a sua precisará de muito poder de computação para executar tarefas de big data, especialmente ao projetar para o futuro, para que você possa superar o crescimento de novos usuários e dados na próxima década. Vamos começar com um exemplo que ilustra como o Google usa seu próprio poder de computação. O Google Fotos recentemente introduziu recursos inteligentes como este, para estabilização de vídeo automática, quando a câmera está instável, como você vê aqui à esquerda. Quais fontes de dados você acha que são necessárias como entradas para o modelo? Bem, você precisa dos dados de vídeo em si, que são basicamente muitas imagens individuais chamadas de frames ordenadas por timestamps. Mas também precisamos de mais dados contextuais do que apenas o próprio vídeo, certo? Absolutamente. Precisamos de dados de séries temporais sobre a posição e orientação da câmera a partir do giroscópio e movimento na lente da câmera. Então, quanto dados de vídeo estamos falando para o modelo ML do Google Fotos para calcular e estabilizar esses vídeos? Se você considerar o número total de valores de ponto flutuante representando um único quadro de vídeo de alta resolução, ele é um produto do número de camadas de canal multiplicadas pela área de cada camada, que com câmeras modernas pode facilmente chegar a milhões. Uma câmera de oito megapixels cria imagens de oito milhões de pixels cada, aproximadamente. Multiplique isso por camadas de três canais e você terá mais de 23 milhões de pontos de dados por quadro de imagem. Existem 30 quadros por segundo de vídeo. Você pode ver rapidamente como um vídeo curto se transforma em mais de um bilhão de pontos de dados para alimentar o modelo. A partir das estimativas de 2018, aproximadamente 1,2 bilhão de fotos e vídeos são enviados ao serviço do Google Fotos todos os dias. São 13 petabytes de dados de fotos no total. Para o YouTube, que também possui modelos de aprendizado de máquina para estabilização de vídeo e outros modelos para transcrevendo áudio. Você está vendo mais de 400 horas de vídeos enviados a cada minuto. São 60 petabytes a cada hora. Mas não é apenas sobre o tamanho de cada vídeo em pixels. A equipe do Google Fotos precisava desenvolver, treinar e fornecer um modelo de aprendizado de máquina de alto desempenho em milhões de vídeos para garantir que o modelo seja preciso. Esse é o conjunto de dados de treinamento para esse recurso. Assim como o hardware de seu laptop pode não ser poderoso o suficiente para processar um trabalho de big data para sua organização, o hardware de um telefone não é poderoso o suficiente para treinar modelos sofisticados de aprendizado de máquina. O Google treina seus modelos de aprendizado de máquina de produção em sua vasta rede de datacenters e, em seguida, implanta versões menores treinadas desses modelos no hardware de seu telefone para previsões em seu vídeo. Um tema comum ao longo deste curso é que, quando o Google faz avanços na pesquisa de IA, continua investindo em novas maneiras de expô-los como modelos totalmente treinados para todos. Você pode, portanto, alavancar a pesquisa de IA do Google com blocos de construção de IA pré-treinados. Por exemplo, se você é uma empresa que produz trailers de filmes e quer rapidamente detectar rótulos e objetos em milhares desses trailers de filmes para criar um sistema de recomendação de filmes, é possível usar uma API do Cloud Video Intelligence. Você poderia usar essa API em vez de criar e treinar seu próprio modelo personalizado. Existem outros modelos totalmente treinados para linguagem e para conversação também. Você aprenderá e praticará o uso desses blocos de construção da IA ​​mais adiante neste curso. Mas voltando à história de aprendizado de máquina do Google Fotos. A execução de muitos modelos ML sofisticados em grandes conjuntos de dados estruturados e não estruturados para os produtos do próprio Google exigiu um investimento maciço em poder de computação. É por isso que a Wired diz: "Isso é o que faz o Google Google. Sua rede física, seus milhares de milhas de fibra e aqueles muitos milhares de servidores que agregam, somam a mãe de todas as nuvens". Em essência, o Google faz computação distribuída há mais de 10 anos para seus próprios aplicativos e, agora, disponibilizou esse poder computacional por meio do Google Cloud. Mas simplesmente escalar o número bruto de servidores nos datacenters do Google não é suficiente. Aqui está um interessante cálculo aproximado de Jeff Dean, que lidera a divisão de inteligência artificial do Google. Ele percebeu anos atrás que, se todos quisessem usar a pesquisa por voz em seus telefones e usá-la por apenas três minutos, precisaríamos duplicar nosso poder de computação. Historicamente, problemas de computação como esse poderiam ser resolvidos pela Lei de Moore. A Lei de Moore foi uma tendência em hardware de computação que descreve a taxa em que o poder de computação dobrou. Durante anos, o poder da computação estava crescendo tão rapidamente que você poderia simplesmente esperar que ela alcançasse o tamanho do seu problema. Embora o poder de computação estivesse crescendo rapidamente, até oito anos atrás, nos últimos anos, o crescimento diminuiu drasticamente à medida que os fabricantes enfrentavam os limites básicos da física. O desempenho de computação atingiu um patamar. Uma solução é limitar o consumo de energia de um chip, e você pode fazer isso criando chips específicos para aplicativos ou ASICs. Um tipo de aplicativo é o aprendizado de máquina. Os novos tipos de hardware projetados pelo Google especificamente para aprendizado de máquina. A Unidade de Processamento Tensor ou TPU é um ASIC especificamente otimizado para ML. Ele tem mais memória e um processador mais rápido para cargas de trabalho ML do que CPUs ou GPUs tradicionais. O Google vem trabalhando no TPU há vários anos e disponibilizou para outras empresas como a sua por meio do Google Cloud problemas de aprendizado de máquina realmente grandes e desafiadores. Uma dessas empresas que usa TPUs é o eBay. Eles usam pods de nuvem da TPU para fornecer inferências mais rápidas, inferências são previsões, inferências mais rápidas para os usuários por um fator de 10. Isso é uma aceleração de 10X. A diminuição no tempo de treinamento do modelo também levou a uma experimentação mais rápida do modelo. O treinamento do modelo ML e a engenharia futura são uma das partes mais demoradas de qualquer projeto de aprendizado de máquina. O vice-presidente de desenvolvimento de novos produtos do eBay observou que a memória adicional dos pods de TPU permitiu que eles melhorassem seu tempo de retorno e iterassem mais rápido. Um último exemplo de poder de computação no Google e uma visão interna da nossa cultura de pensar 10X é como nossas equipes usaram o aprendizado de máquina para aumentar a eficiência do próprio datacenter do Google. O impacto potencial para o aprendizado de máquina estava lá, considerando o número de datacenters que o Google precisa manter refrigerado e alimentado, e já estávamos coletando dados do sensor de fluxo para nossas plataformas de monitoramento existentes. Os engenheiros e a mente profunda da Alphabet viram isso como uma oportunidade de ingerir esses dados de sensores e treinar um modelo de aprendizado de máquina para otimizar o resfriamento melhor do que os sistemas existentes. O modelo implementado reduziu a energia de resfriamento em 40% e aumentou a eficácia geral da energia em 15%. Considero este exemplo particularmente inspirador, porque é um modelo de aprendizado de máquina treinado em hardware especializado em aprendizado de máquina em um datacenter, dizendo ao datacenter o quão difícil ele pode executar o aprendizado de máquina especializado no qual o modelo está treinando. Poderoso material de nível de iniciação. Posteriormente no curso, você verá uma demonstração sobre como configurar um pipeline de processamento de dados de streaming para seus próprios dispositivos de IoT em menos de uma hora.


>>> Video 4 <<< 13 minutos.

Então, nesta demonstração, mostrarei como criar uma instância do Compute Engine, executar alguns softwares nela, copiar arquivos dessa instância do Compute Engine para o armazenamento na nuvem e publicar esses arquivos para torná-los públicos. O caso de uso que vamos fazer é traçar a atividade do terremoto que aconteceu na semana anterior, e esses dados vêm do USGS. Então deixe-me ir em frente e vá em cloud.google.com. Por isso, estou começando em cloud.google.com e vou para a parte do Compute Engine dele, acesse as instâncias de VM e crie uma VM. Então, clico em Criar aqui. Neste ponto, estou criando uma nova máquina virtual, vamos chamar de "VM do terremoto". Você pode chamar o que quisermos. Podemos escolher onde no mundo queremos. Por acaso estou mais perto de Oregon agora, então eu vou em frente e faço o oeste dos EUA e eu vou escolher uma zona, eu posso escolher US-West-B. Deixe-me ir em frente e dizer que eu quero usar duas CPUs virtuais. Eu posso, como você pode ver, podemos neste momento subir para cerca de 64 CPUs com cerca de 250 GB de RAM. Outra coisa que posso fazer é clicar em Customize e dizer que quero 18 CPUs e 50 GB de RAM. Vá me encontrar uma máquina que corresponda a esses requisitos. Mas no nosso caso, o que nós vamos fazer, é que vamos voltar e escolher uma máquina bem pequena como uma vCPU é provavelmente o suficiente, e então 10 shows de espaço são suficientes, mas é claro que podemos mudar isso e nós pode usar um sistema operacional diferente com o tamanho diferente do disco, se quiséssemos. Queremos usar o gsutil G Cloud, etc. Por isso, vou permitir acesso total a todas as minhas APIs na nuvem, para que eu possa gravar no Cloud Storage a partir da VM. Eu não quero acessar diretamente a VM através de HTTP ou HTTPS. Então agora vamos apenas acessá-lo através do SSH. Então, vou em frente e criar a máquina virtual e, neste momento, a máquina virtual está sendo criada. Vamos aguardar alguns segundos e, quando a VM estiver funcionando, poderemos usar o SSH nela. Então é isso. Aqui está minha VM e, neste momento, essa é uma máquina virtual básica na qual eu sou o SSH. Esta máquina virtual, por isso não tem nada nela. Na verdade, ele não tem nem mesmo softwares básicos como o Git. Então, se eu fosse digitar Git aqui, ele diz "Git não foi encontrado". Então eu preciso instalar o Git. Ainda bem que podemos fazer isso. Eu tenho acesso root. Então, o que eu posso fazer é, eu posso fazer o sudo que me dá acesso root apt-get install git. 
Diz. "Você quer continuar?" Sim eu quero. Isso agora instala o Git, que é uma maneira de acessar meu repositório de código-fonte que contém tudo o que preciso. Então, vou em frente e clit git https://www.github.com/GoogleCloudPlatform.
3:33
Então isso agora pega os dados do GitHub, obtém todos os dados que eu preciso incluindo os arquivos. Agora, se eu fizer o LS, eu tenho meu repositório. Neste caso, isso faz parte dos arquivos deste curso e, portanto, podemos entrar nos fundamentos do bdml, e há uma VM de terremoto. Eu vou para a VM do terremoto e vejo quais arquivos estão presentes. Acontece que eu já tenho um arquivo HTML. Eu tenho algum script de shell para ingerir dados. Eu posso fazer menos ingest.sh e isso basicamente segue em frente e remove um terremotos existente.csv, e faz um wget que é uma maneira de baixar dados via HTTP de earthquake.usgs.gov, e salva isso como earthquakes.csv. Então, é isso que ingest dataset faz. Então, eu também tenho transform.py e transform.py basicamente, é algum código Python que usa o Matplotlib para ir em frente e analisar os dados do terremoto do USGS e colocá-lo em um mapa da Terra, cria um PNG fora dele. Portanto, esse arquivo em particular pode ser executado para prosseguir e processar dados semanais do USGS. Ele vai em frente e recebe uma semana de dados do USGS e cria earthquakes.png. A fim de fazer isso, porém, eu preciso ter algum software instalado e o software que eu preciso, está listado na instalação missing.sh. Então eu preciso basicamente pegar um monte de bibliotecas Python diferentes. Então deixe-me ir em frente e primeiro execute install missing.sh. Isso vai em frente e faz o download e instala todos os pacotes do Python que eu preciso. Como você pode ver, trabalhar com uma máquina virtual na nuvem é como trabalhar com qualquer outra máquina Unix nesse caso, porque criei uma máquina DBN, tenho uma máquina Unix e essa é uma máquina Unix com a qual estou trabalhando. Neste ponto, ele tem todo o Python pacotes que eu quero. Nós podemos ir em frente e dizer ingest.sh, e ele irá em frente e baixará earthquakes.csv. Você pode ver que há um terremoto.csv agora, que foi criado. Se fôssemos fazer head, head mostrará as primeiras linhas do arquivo. Acontece que atualmente tem havido um terremoto na Califórnia, e houve outro terremoto no Alasca e assim por diante. Então, vamos em frente, e há 93 quilômetros a leste a nordeste de Cape Yakataga, no Alasca, no momento em que estou gravando essa demo. Então, há um monte de terremotos. Nós obtivemos os dados, mas olhar para um arquivo CSV não é muito interessante. Vamos em frente e criar uma imagem com isso. Para fazer isso, vou executar o transform.py. Então, transform.py, está essencialmente obtendo os dados do terremoto, convertendo uma imagem, e percebe que neste ponto não há imagem, mas agora, se eu fizer isso, eu agora tenho um earthquakes.png. Então, vamos em frente e agora colocá-lo na nuvem. Queremos nos livrar dessa máquina virtual. A ideia é computar e o armazenamento é separado. Então, fizemos o cálculo, temos alguns arquivos, vamos copiá-los da máquina para o armazenamento em nuvem, para que possamos excluir a máquina. Então, vamos criar um intervalo de armazenamento em nuvem. A maneira de fazer isso, é novamente descer aqui. Da última vez que fizemos o Compute Engine, desta vez vamos fazer o Storage. Então, vamos ao navegador de armazenamento e dizemos que queremos criar um bloco. Então, vamos em frente, eu já criei esse intervalo, deixe-me excluir isso, não preciso desse intervalo. Digamos que eu queira criar um balde e eu tenho que dar um nome exclusivo ao meu balde. Então, digamos que eu queira dar ao balde o nome "Terremoto". Mas o nome do intervalo já está em uso. Um nome de intervalo precisa ser globalmente exclusivo. Então eu posso dizer "earthquake2019" e eu tenho isso. Portanto, não há mais ninguém que tenha criado um balde com esse nome. Se você não conseguir criar um nome exclusivo, tente usar um nome de projeto. Esse nome de projeto também é globalmente exclusivo e, em muitos casos, pode funcionar também. Uma maneira fácil de fazer isso, é que podemos ir até o menu inicial e há um nome de projeto, eu posso copiar o nome do projeto, voltar para o armazenamento e dizer: "Posso criar um intervalo com esse nome?" Sim eu posso. Então isso também funciona. Então, posso criar algo com esse nome específico como meu balde. Vamos torná-lo multi-regional, para que possamos acessá-lo de qualquer lugar do mundo, e isso vai nos custar cerca de US $ 0,03 por 

gigabyte por mês. Agora podemos escolher como definir as permissões. Normalmente, você deseja definir permissões, mas um objeto por objeto. Mas vamos em frente e especificá-lo objeto por objeto, para que possamos seguir em frente e criar o intervalo. Então, agora, criamos um bloco com esse nome e copiamos isso para o meu negócio, e esse intervalo agora é criado. Agora podemos voltar para essa máquina virtual e podemos dizer gsutil ls e o nome do intervalo e não tem nada agora. Então deveria estar vazio. Então não tem nada nele. O que podemos fazer agora é que temos gsutil copia earthquakes.star para o balde.
9:46
Então, neste ponto, ele copiou três arquivos, copiou o arquivo CSV, o arquivo HTM e o arquivo PNG. De fato, quando chegarmos aqui e dissermos "Refresh", deveríamos ter os três arquivos. Então viva. Então, nós temos os três arquivos que todos os três arquivos atualmente não são públicos, então ninguém pode acessá-los, exceto nós, porque nós possuímos esses arquivos. Mas, a partir de agora, não preciso da máquina virtual, portanto, basta ir até o mecanismo de computação e dizer "Ei, essas instâncias de VM virtual" selecione minha máquina virtual e posso fazer uma de duas coisas. Eu posso parar a máquina virtual, nesse caso é como pausar a VM. Pagarei pelo disco, mas não pagarei pelo armazenamento. A outra coisa que posso fazer em vez de parar a VM é que posso excluir a VM. Então eu posso ir em frente e excluir a VM, eu não preciso mais dela novamente. Então parar é realmente uma maneira muito conveniente, nós instalamos um monte de software, não queremos nos livrar da VM, mas não queremos pagar pelo cálculo da VM. Então, o que poderíamos fazer é, poderíamos simplesmente parar. Portanto, a VM não está mais em execução e podemos apenas dizer "Parar" e agora a VM está pausada. Neste ponto, quando a VM for pausada, só seremos cobrados pelos 10 GB de disco que anexamos à VM. Então, maneira relativamente barata de ter uma máquina virtual por aí que conhece todo o software tudo o que temos instalado nele. Se precisarmos executar as coisas novamente, podemos voltar, reiniciar a VM e executar as coisas. Mas vamos voltar e voltar ao nosso armazenamento. Você verá que os dados aqui são privados e o que queremos fazer é que queremos torná-los públicos, queremos torná-los acessíveis a qualquer pessoa. O que poderíamos fazer é podermos ir ao armazenamento.

Agora, estamos no segmento, e podemos dizer que queremos pegar esses três arquivos, ir para Permissões e podemos dizer que queremos ir para as permissões e queremos adicionar membros. Podemos adicionar membros chamados allUsers e dar a esses usuários, a todos, o visualizador de objetos de armazenamento. Então eles poderão ver esse objeto em particular. Portanto, agora temos nossos allUsers e allUsers tem Storage Object Viewer nesses objetos. Então, agora, se você for agora, vemos que essas três coisas são públicas e a Nuvem nos adverte que essas três coisas são públicas, todas elas têm links públicos associados a elas. Então, podemos ir até aqui e dizer: "O que é um link público?" Clique no link, que é a API de armazenamento, que é o GoogleAPIs.com. Então, qualquer um que chegar a este link em particular poderá ver basicamente os terremotos desta semana. Então, neste momento, estou gravando em 4 de abril, então a semana começou em 28 de março. Então eu tenho sete dias de dados e estes são os terremotos que vimos no Alasca, e esses são os terremotos que vimos na Califórnia, e você pode ver o Anel de Fogo onde todos esses terremotos estão acontecendo ao redor da costa do Pacífico. Mas a linha de fundo, o que fizemos? Criamos uma máquina virtual, realizamos o processamento e, em seguida, pegamos esses arquivos e os copiamos para o armazenamento em nuvem. Conseguimos parar a VM, a VM não está mais em execução, mas os arquivos ainda estão disponíveis e podemos atendê-los.


>>> Video 5 <<< 5 minutos.

Na demonstração no final da seção anterior, copiei os arquivos ingeridos e transformados da instância de computação para o Cloud Storage. Dessa forma, poderíamos parar nossa instância de computação e manter o acesso aos dados. Embora tenhamos discutido a demanda por uma grande quantidade de capacidade de computação para os grandes volumes de dados e trabalhos de ML, ainda precisamos de um local para armazenar todos os dados gerados. Assim como no demo, isso precisa ser separado das instâncias de computação, para que possamos analisá-lo, transformá-lo e alimentá-lo em nossos modelos. Essa é uma maneira importante de a computação em nuvem diferir da computação em desktop. Computação e armazenamento são independentes. Você não quer pensar em discos anexados à instância de computação como o limite de quantos dados você pode processar e armazenar. Colocar seus dados em sua solução e transformá-los para seus objetivos, deve ser sua primeira prioridade. Nos papéis e nas discussões estruturadas da equipe sobre as quais falarei mais adiante neste módulo, falarei sobre a necessidade de os engenheiros de dados criarem pipelines de dados antes que você possa começar a construir modelos de aprendizado de máquina a partir desses dados. Quando o pipeline é construído, o trabalho não é feito. Como os dados estão no sistema, os engenheiros de dados ainda precisam replicar os dados, fazer o backup, dimensioná-los, removê-los conforme necessário, tudo em grande escala. Em vez de engenheiros de dados gerenciarem as próprias infra-estruturas de armazenamento, os engenheiros de dados podem usar o Google Cloud Storage, que é um sistema de arquivos global durável. Criando um depósito de armazenamento elástico, é tão simples quanto usar a WebUI em console.cloud.google.com ou a função do utilitário Armazenamento do Google, chamada gsutil, na interface da linha de comando. Como um nível adicional de flexibilidade, um engenheiro de dados pode escolher o tipo de classe de armazenamento que deseja para seus dados. Existem quatro classes de armazenamento para você escolher com base nas suas necessidades de dados: multirregional, regional, nearline e coldline. Para cargas de trabalho analíticas de big data, o mais comum é usar um repositório de armazenamento em nuvem regional para testar seus dados. Como mencionamos um recurso do GCP, um intervalo do Cloud Storage, é importante cobrir parte da logística do gerenciamento de contas antes de nos aprofundarmos. A partir dos objetos mais granulares, você vê recursos como um intervalo do Cloud Storage ou uma instância do Compute Engine , esses recursos pertencem a projetos específicos. Balde os nomes precisam ser globalmente exclusivos e o GCP também atribui seu ID de projeto que é globalmente exclusivo e, portanto, você pode usar esse ID de projeto como um nome exclusivo para o seu intervalo. Mas o que é um projeto? Um projeto é uma entidade organizadora de nível básico para criar e usar recursos e serviços para gerenciar APIs e permissões de faturamento. Zonas e regiões organizam fisicamente os recursos do GCP que você usa. Considerando que os projetos organizam logicamente. Os projetos podem ser criados, gerenciados, excluídos e até mesmo recuperados de exclusões acidentais. Pastas são outro agrupamento lógico, você pode ter para coleções de projetos. Ter uma organização é necessário para usar pastas. Mas o que é uma organização? A organização é um nó raiz de toda a hierarquia do GCP. Embora não seja obrigatório, uma organização é bastante útil, pois permite que você defina políticas que se apliquem em toda a empresa a todos os projetos e todas as pastas criadas em sua empresa. O Cloud Identity e o Access Management, também chamado de IAM ou IAM, permitem ajustar o controle de acesso a todos os recursos do GCP que você usa. Você define políticas do IAM que controlam o acesso do usuário aos recursos. Lembre-se, se você quiser usar pastas, você deve ter uma organização. Agora que você criou um intervalo do Google Cloud Storage, como você obtém seus dados na nuvem e trabalha com os dados quando eles estão no bucket? Na demonstração, usei os comandos da gsutil. Especificamente, podemos usar cp para cópia e especificar um local de bucket de destino. Se você criar uma instância do Compute Engine, a ferramenta de linha de comando gsutil já estará disponível e poderemos fazer a cópia da gsutil. No seu laptop, você pode fazer o download do Google Cloud SDK para obter a gsutil. A Gsutil usa uma sintaxe familiar de linha de comando do Unix. Então, se você sabe como usar o comando cp no Unix, você sabe como usar o gsutil cp.



>>> Video 6 <<< 3 minutos.

Até agora, analisamos o cálculo e analisamos o armazenamento. A terceira parte da infraestrutura do Google Cloud é a rede. A rede privada de alta qualidade do Google, a largura de banda bipectional do petabit e os pontos de presença do Edge são combinados usando uma rede definida por software de última geração para fornecer uma solução poderosa. Primeiro, a rede privada. O Google lançou milhares de quilômetros de cabos de fibra óptica que cruzam oceanos com repetidores para amplificar sinais ópticos, e, como você pode ver neste divertido GIF, é à prova de tubarões. Centros de dados do Google em todo o mundo estão interconectados por essa rede privada do Google, que, segundo algumas estimativas disponíveis publicamente, transporta até 40% do tráfego mundial de internet todos os dias. Esta é a maior rede desse tipo na Terra e continua crescendo. Em segundo lugar, a largura de banda bipectional do petabit. Uma das equipes que discutiremos neste curso é uma separação entre computação e armazenamento. Você não precisa mais fazer tudo em uma única máquina ou até mesmo em um único cluster de máquinas com seu próprio armazenamento dedicado. Por quê? Bem, se você tiver uma rede suficientemente rápida, poderá realizar cálculos em dados localizados em outros lugares, como muitos servidores distribuídos. A Jupiter Network do Google pode fornecer largura de banda suficiente para permitir que 100.000 máquinas se comuniquem entre si. Então, para qualquer máquina se comunicar com qualquer outra máquina no data center em mais de 10 gigabits por segundo. Essa largura de banda full duplex significa que a localidade dentro do cluster não é importante. Se cada máquina puder falar com qualquer outra máquina a 10 Gbps, os racks não são importantes para análise de dados e aprendizado de máquina. Mas você precisa ingerir dados provavelmente de todo o mundo. Você precisa exibir os resultados de suas análises e previsões, talvez para usuários que estão em todo o mundo. É aí que entra a Edge points of presence. A rede, a Rede do Google, interconecta-se com a Internet pública em mais de 90 trocas de internet e mais de 100 pontos de presença em todo o mundo. Quando um usuário da Internet envia tráfego para um recurso do Google, o Google responde à solicitação do usuário de um Edge 
local de rede que fornecerá o menor atraso ou latência. A rede de cache do Google Edge coloca o conteúdo perto dos usuários finais para minimizar a latência. Seus aplicativos no GCP, como seus modelos de aprendizado de máquina, também podem aproveitar essa rede do Edge.



>>> Video 7 <<< 2 minutos.


A última peça da infraestrutura básica que sustenta seus pipelines de dados e modelos de aprendizado de máquina é a grande segurança do Google. Quando você cria um aplicativo em sua infraestrutura local, você é responsável pela segurança da pilha inteira, da segurança física do hardware e das instalações nas quais eles estão hospedados por meio da criptografia dos dados no disco, a integridade de sua rede e até proteger o conteúdo armazenado nesses aplicativos. Mas quando você move um aplicativo para o GCP, o Google lida com muitas das camadas inferiores de segurança, como a segurança física do hardware e de suas instalações, a criptografia de dados no disco e a integridade da rede física. Por causa de sua escala, o Google pode oferecer um nível mais alto de segurança nessas camadas, do que a maioria dos clientes poderia pagar por conta própria. As camadas superiores da pilha de segurança, incluindo a proteção de dados, permanecem de sua responsabilidade. Mas mesmo aqui, o Google fornece ferramentas como o Cloud IAM para ajudar você a implementar as políticas definidas nessas camadas. As comunicações pela Internet para nossos serviços de nuvem pública são criptografadas em trânsito. A rede e a infraestrutura do Google têm várias camadas de proteção para defender nossos clientes contra ataques de negação de serviço. Os dados armazenados são automaticamente criptografados em repouso e distribuídos para disponibilidade e confiabilidade. Isso ajuda a proteger contra acesso não autorizado e interrupções de serviço. Um produto específico que destacarei aqui, que você verá muito nesse curso, é o BigQuery. Armazém de dados de análise de escala de petabytes do Google Cloud. Os dados em uma tabela do BigQuery são criptografados usando uma chave de criptografia de dados. Então, essas chaves de criptografia de dados são criptografadas com chaves de criptografia de chaves. Isso é conhecido como criptografia de envelope e fornece segurança adicional. O BigQuery também permite que você forneça suas próprias chaves de criptografia. Essas são chamadas de chaves de criptografia gerenciadas pelo cliente. Por dentro do BigQuery, você pode monitorar o uso do BigQuery da sua equipe e executar consultas e limitar de maneira proativa o acesso a dados em uma linha e em um nível de coluna. Cobrimos o BigQuery como um serviço com mais detalhes posteriormente. Porém, nesta seção, queremos apenas destacar os controles de segurança do BigQuery como um exemplo dos controles de segurança que você encontrará em todos os serviços do GCP.




>>> Video 8 <<< 2 minutos.

Até agora, falamos sobre infraestrutura, computação, armazenamento, rede e segurança de baixo nível. No entanto, como engenheiro de dados ou cientista de dados ou analista de dados, você normalmente trabalha com produtos de nível superior. Então, vamos falar sobre os produtos de big data e de aprendizado de máquina que formam o Google Cloud Platform. Falaremos sobre a cronologia da inovação não pela história, mas para que você entenda a evolução das estruturas de processamento de dados. Saber como essas estruturas evoluíram pode ajudar você a entender os problemas típicos que surgem e como eles são abordados. Uma das coisas interessantes sobre o Google é que, historicamente, enfrentamos problemas relacionados a grandes conjuntos de dados, dados que mudam rapidamente e dados variados, o que é comumente chamado de big data mais cedo do que o restante do setor. Ter de indexar uma World Wide Web fará isso, e assim como a Internet cresceu, o Google inventou novos métodos de processamento de dados. Em 2002, o Google criou o GFS, ou o sistema de arquivos do Google, para lidar com fragmentação e armazenamento de petabytes de dados em grande escala. O GFS é a base do armazenamento em nuvem e também do que seria o armazenamento gerenciado pelo BigQuery. Um dos próximos desafios do Google foi descobrir como indexar o volume explosivo de conteúdo na web. Para resolver isso em 2004, o Google inventou um novo estilo de processamento de dados conhecido como MapReduce para gerenciar o processamento de dados em larga escala em grandes clusters de servidores de commodity. Os programas MapReduce são automaticamente paralelizados e executados em um grande cluster dessas máquinas de commodities. Um ano depois de o Google publicar um white paper descrevendo a estrutura do MapReduce, Doug Cutting e Mike Cafarella criaram o Apache Hadoop. O Hadoop ultrapassou muito seu início na indexação da Web e agora é usado em muitos setores para uma enorme variedade de tarefas que compartilham o tema comum de volume, velocidade e variedade de dados estruturados e não estruturados. Com o crescimento das necessidades do Google, enfrentamos o problema de registrar e recuperar milhões de ações do usuário de streaming com alto throughput, que se tornou o Cloud Bigtable, que foi uma inspiração para o Hbase ou o MongoDB. Um problema com o MapReduce é que os desenvolvedores precisam escrever código para gerenciar toda a infraestrutura de servidores de commodity. Os desenvolvedores não podiam se concentrar apenas na lógica de sua aplicação.

Então, entre 2008 e 2010, o Google começou a se afastar do MapReduce para processar e consultar grandes conjuntos de dados, e em vez disso eles começaram a se mover em direção a novas ferramentas. Ferramentas como a Dremel. A Dremel adotou uma nova abordagem para o processamento de big data, em que a Dremel divide os dados em pequenos fragmentos chamados shards e os compacta em um formato colunar em armazenamento distribuído. Em seguida, ele usa um otimizador de consulta para realizar o farm de tarefas entre os muitos fragmentos de dados e os datacenters do Google com hardware de commodity para processar uma consulta em paralelo e entregar os resultados. O grande salto para a frente aqui foi que o serviço, os automanagers desequilibram os dados e as comunicações entre os funcionários, e os autodesenvolvem para atender a diferentes demandas de consulta e, como você verá em breve, a Dremel tornou-se o mecanismo de consulta por trás do BigQuery. O Google continuou a inovar para resolver seus grandes desafios de dados e ML, e criou o Colossus como um armazenamento de dados distribuídos de última geração, a Spanner como um banco de dados relacional em escala planetária. Flume e Millwheel para pipelines de dados, Pub / Sub para mensagens, TensorFlow para aprendizado de máquina, mais há hardware especializado TPU que vimos anteriormente, e ML Auto que virá mais tarde. A boa notícia para você é que o Google abriu essas inovações como produtos e serviços para você aproveitar como parte da plataforma do Google Cloud. Você vai praticar o trabalho com esses produtos em seus laboratórios como parte deste curso básico.



>>> Video 9  <<< 10 minutos.

O Google Cloud hospeda mais de 150 conjuntos de dados públicos no BigQuery para exploração e uso em aplicativos

Faça de cinco a dez minutos e leia sobre o programa aqui e continue com seu primeiro laboratório, que explorará um conjunto de dados público no BigQuery

Recursos adicionais:

Casos de uso do BigQuery
Clientes do Google Cloud que usam ferramentas de Big Data.


O BigQuery é o armazenamento de dados empresariais altamente escalonável e sem servidor do Google, desenvolvido para tornar os analistas de dados mais produtivos, com um custo-benefício incomparável. Como não há infraestrutura para gerenciar, concentre-se em gerar insights significativos com um SQL conhecido, sem a necessidade de ter um administrador de banco de dados.

Analise todos os seus dados em lote e de streaming por meio da criação de um armazenamento de dados lógico a partir do armazenamento gerenciado em colunas, bem como de dados do armazenamento de objetos e de planilhas. Crie painéis e relatórios incrivelmente rápidos com o BI Engine na memória. Crie e operacionalize soluções de machine learning ou realize análises geoespaciais usando um SQL simples. Compartilhe insights com segurança, dentro e fora da sua organização, em formatos de conjuntos de dados, consultas, planilhas e relatórios. Com o processamento avançado de streaming do BigQuery, é possível capturar e analisar dados em tempo real para garantir insights sempre atuais. Além disso, você tem até 1 TB gratuito de dados analisados por mês e 10 GB de dados armazenados.

Concentre-se na análise e não na infraestrutura
Configuração rápida
Comece a trabalhar rapidamente
Configure seu armazenamento de dados em segundos e comece a consulta imediatamente. O BigQuery executa consultas SQL extremamente rápidas em volumes de dados que vão de gigabytes a petabytes. Além disso, facilita a combinação de conjuntos de dados públicos ou comerciais com seus dados. Treine um modelo de machine learning em minutos ou conduza análises geoespaciais com SQL simples. Processe dados de streaming e visualize insights. Elimine a demorada tarefa de provisionamento de recursos e reduza o tempo de inatividade com uma infraestrutura sem servidores que se encarrega de toda a manutenção, inclusive de patches e upgrades. O BigQuery usa um SQL familiar, em conformidade com o ANSI, e fornece drivers ODBC e JDBC para tornar a integração com seus dados rápida e fácil.

Escalonamento sem problemas
Escalonamento sem problemas
O BigQuery separa armazenamento e computação para ativar o escalonamento elástico que simplifica o planejamento da capacidade de armazenamento de dados. O BigQuery enfrenta os desafios da análise em tempo real com a ajuda da infraestrutura sem servidor do Google, que usa dimensionamento automático e processamento de streaming de alto desempenho para carregar dados. O armazenamento gerenciado em colunas, a execução paralela em massa e as otimizações automáticas de desempenho do BigQuery capacitam os usuários a analisar dados de maneira rápida e simultânea com base no seu data lake na nuvem, independentemente do número de usuários ou do tamanho dos dados.

Insights mais rápidos, com análises avançadas
Insights mais rápidos, com análises avançadas
Tenha uma visão completa de todos os seus dados por meio de consultas ao armazenamento gerenciado em colunas do BigQuery, ao Cloud Storage, ao Cloud Bigtable, ao Planilhas e ao Drive. O BigQuery se integra a ferramentas de ETL atuais, como Informatica e Talend, para enriquecer seus dados com o DTS. Também oferece suporte integrado a ferramentas de BI conhecidas, como Tableau, MicroStrategy, Looker e Data Studio. Assim, qualquer pessoa pode criar facilmente planilhas e relatórios incríveis. Processe e visualize automaticamente dados de marketing e do Google Ads usando o serviço de transferência de dados do BigQuery para configurar um armazenamento de dados de marketing avançado com apenas alguns cliques. Por meio do serviço de transferência de dados do BigQuery, os usuários também conseguem acesso a conectores de dados que facilitam a transferência de dados do Teradata e do Amazon S3 para o BigQuery.

Proteja os dados da sua empresa
Proteja os dados e os investimentos da sua empresa
Com o BigQuery, é fácil manter uma segurança confiável e a base para a governança. Elimine a sobrecarga das operações de dados com a replicação de dados automática para recuperação de desastres e alta disponibilidade de processamento sem custos extras. O BigQuery oferece um SLA de 99,9% sujeito aos termos descritos aqui e obedece aos princípios do Privacy Shield.

Controle os custos
Controle os custos e reduza o TCO
Pague somente pelos recursos de armazenamento e computação que usar, graças à arquitetura sem servidor do BigQuery. A separação de armazenamento e computação do BigQuery facilita o escalonamento independente e contínuo sob demanda, resultando em baixo custo e armazenamento mais econômico. Com o BigQuery, você reduz o custo total de propriedade em 56% a 88%.

Nossos clientes
logotipo da home depot
logotipo do hsbc
logotipo da sky news
logotipo da helegraph media group
logotipo de heathrow
logotipo do spotify
logotipo do newyork times
Recursos
Sem servidor	Com o armazenamento de dados sem servidor, o Google faz todo o provisionamento de recursos em segundo plano para que você possa se concentrar nos dados e na análise, em vez de se preocupar em atualizar, proteger ou gerenciar a infraestrutura.
Análise em tempo real	A API de inserção de streaming de alta velocidade do BigQuery oferece uma base poderosa para a análise em tempo real. Seus dados empresariais mais recentes se tornam imediatamente disponíveis para análise.
Alta disponibilidade automática	O BigQuery oferece armazenamento replicado e durável, de maneira transparente, automática, em vários locais e com alta disponibilidade, sem custos extras e sem necessidade de qualquer outra configuração.
SQL padrão	O BigQuery é compatível com um dialeto SQL padrão, em conformidade com o ANSI:2011, o que reduz a necessidade de reescrever códigos. O BigQuery também oferece drivers ODBC e JDBC sem custo, para garantir que seus aplicativos atuais possam interagir com esse poderoso mecanismo.
Consultas federadas e serviço lógico de armazenamento de dados	Usando consultas federadas avançadas, o BigQuery pode processar fontes de dados externas em armazenamentos de objetos (Cloud Storage), bancos de dados transacionais (Cloud Bigtable) ou planilhas no Google Drive. Tudo isso sem duplicar os dados.
Separação de armazenamento e computação	Com a separação entre os recursos de armazenamento e computação do BigQuery, escolha as soluções de armazenamento e processamento que atendam às necessidades da empresa e controle o acesso e o custo para cada uma delas.
Backup automático e fácil restauração	O BigQuery replica dados automaticamente e mantém um histórico das alterações dos últimos sete dias, o que permite a você restaurar e comparar facilmente dados de momentos diferentes.
Tipos de dados e funções geoespaciais	O BigQuery GIS oferece suporte SQL para pontos arbitrários, linhas, polígonos e multipolígonos nos formatos WKT e GeoJSON. É possível simplificar as análises geoespaciais, ver seus dados com base em localização de uma nova maneira ou abrir linhas de negócios totalmente novas.
Serviço de transferência de dados	O serviço de transferência de dados do BigQuery transfere automaticamente para o BigQuery os dados de fontes de dados externas, como Google Marketing Platform, Google Ads, YouTube e aplicativos SaaS de parceiros, de maneira programada e gerenciada. Os usuários também conseguem transferir facilmente os dados do Teradata e do Amazon S3 para o BigQuery.
Integração do ecossistema de Big Data	Em conjunto com o Cloud Dataproc e o Cloud Dataflow, o BigQuery oferece integração com o ecossistema de Big Data do Apache, permitindo que suas cargas de trabalho atuais do Hadoop/Spark e do Beam leiam ou gravem dados diretamente a partir do BigQuery.
Escala de petabytes	Consiga um excelente desempenho dos seus dados, além de contar com a capacidade de escalonamento para armazenar e analisar mais petabytes sem precisar comprar mais armazenamento.
Modelos de preços flexíveis	O modelo sob demanda permite que você pague apenas pelo armazenamento e os recursos de computação que utilizou de fato. No modelo fixo, os usuários ou as empresas com alto volume de dados podem escolher um custo mensal estável. Para mais informações, consulte Preços do BigQuery ou controle de custos.
Governança e segurança de dados	Com o BigQuery, é fácil manter uma segurança confiável e um gerenciamento minucioso de identidade e acesso por meio do Cloud Identity and Access Management. Seus dados estão sempre criptografados, estejam em repouso ou em trânsito.
Geoexpansão	O BigQuery oferece a opção de controle geográfico de dados (em locais nos Estados Unidos, Ásia e Europa), sem a preocupação de configurar e gerenciar clusters e outros recursos de computação regionais.
Base para inteligência artificial	Além de levar o machine learning até seus dados com o BigQuery ML, a integração com o Cloud ML Engine e o TensorFlow permite treinar modelos avançados em dados estruturados, em minutos e só com SQL.
Base para BI	O BigQuery forma a parte essencial do armazenamento de dados das soluções de BI modernas e oferece integração de dados, transformação, análise, visualização e geração de relatórios integrados, com ferramentas do Google e dos nossos parceiros de tecnologia.
Processamento de dados flexível	Carregue seus dados do Cloud Storage ou transmita-os ao BigQuery em uma velocidade de milhares de linhas por segundo para possibilitar a análise em tempo real. Comece imediatamente a usar ferramentas de integração de dados conhecidas, como Informatica, Talend e outras prontas para uso.
Interação programática	O BigQuery oferece uma API REST para facilitar o acesso programático e a integração de aplicativos. Estão disponíveis bibliotecas de cliente para Java, Python, Node.js, C#, Go, Ruby e PHP. Usuários corporativos podem usar o Google Apps Script para acessar o BigQuery a partir do Planilhas.
Monitoramento e criação de registros avançados com o Stackdriver	O BigQuery oferece recursos avançados de monitoramento, criação de registros e alertas por meio dos registros de auditoria do Stackdriver e pode operar como repositório para registros de qualquer aplicativo ou serviço que use o Stackdriver Logging. 
Principais recursos
BIGQUERY ML
BIGQUERY BI ENGINE
BIGQUERY GIS
GIF do BigQuery ML
BigQuery ML (Beta)
Usando um SQL simples, o BigQuery ML permite que cientistas e analistas de dados criem e operacionalizem modelos de machine learning em dados estruturados e semiestruturados em escala global, diretamente no BigQuery. Tudo isso em uma fração de tempo.

Crie, teste e operacionalize modelos personalizados de machine learning usando o SQL que você já conhece.
Crie modelos de machine learning em minutos, diretamente no BigQuery, sem amostragem extensa ou migração de dados do armazenamento.
Promova as recomendações, segmentações e previsões de produtos em escala de petabytes e com custos bem menores.




>>> Video 10  <<< 3 minutos.

Olá e bem-vindo. Sou Philipp Maier, desenvolvedor de cursos do Google Cloud Platform. Este é um breve tutorial sobre o uso do Qwiklabs neste curso. Estou prestes a mostrar a você a plataforma interativa do laboratório prático chamada Qwiklabs, que faz parte do Google Cloud. O Qwiklabs permite que você tenha uma experiência prática com o GCP e garanta suas credenciais de conta do Google, para que você possa acessar um console do GCP sem nenhum custo. O primeiro passo é entrar no Coursera em uma janela anônima. Dependendo do seu navegador, ele também pode ser chamado de navegação privada ou navegação privada. Fazer o login no Coursera a partir de uma janela privada, garante que você não use acidentalmente sua própria conta do Google enquanto acessa o Google Cloud Console. Não queremos que você receba nenhuma fatura inesperada no final do mês. Confira os links abaixo deste vídeo para artigos de suporte para navegadores diferentes. Uma vez logado no Coursera usando uma janela anônima, retorne ao seu curso e vá para a página de atividade do laboratório. Então, se solicitado, você deseja aceitar o código de honra e talvez tenha que digitar seu nome. Em seguida, você deseja clicar no botão Abrir ferramenta para abrir o laboratório em uma nova guia. Portanto, na nova guia, você pode clicar em Iniciar Laboratório. Você quer esperar até que o laboratório seja exibido.
1:25
Para cada laboratório, você terá um temporizador no canto superior direito com o tempo de acesso restante. Seu laboratório terminará automaticamente quando o tempo acabar. À esquerda, você tem o detalhe das conexões. Clique em Abrir o Google Console e, em seguida, entre com o nome de usuário e a senha fornecidos no painel de detalhes das conexões. Então, vou copiar o nome de usuário. Cole isso aqui. Eu também vou levar a senha. Cole aqui também. Agora, a Qwiklabs cria uma nova conta para você toda vez que você lança um laboratório. Portanto, você precisa clicar nas janelas iniciais de configuração da conta. Então, essencialmente, eu preciso aceitar isso. Não preciso adicionar nenhum número de telefone recuperado, por isso, basta clicar em Concluído.


Eu vou concordar com os termos dos serviços e não preciso de nenhum e-mail.
2:24
Agora, posso verificar se estou usando a conta e o projeto fornecidos pela Qwiklabs no console do GCP. Então, aqui em cima, vejo o ID do projeto. Por aqui, posso ver o nome de usuário. Estes são os mesmos que foram fornecidos no painel de detalhes de conexões. Você também pode ver que o temporizador ainda está em execução. Agora, alguns laboratórios acompanham seu trabalho dentro do projeto GCP fornecido pela Qwiklabs. Se isso estiver ativado, você verá uma pontuação no canto superior direito da janela do Qwiklabs, como você pode ver aqui. Sua pontuação aumenta à medida que os objetivos são atingidos e você pode clicar na pontuação para ver as etapas individuais a serem pontuadas. Agora que concluí o laboratório, vejo que minha pontuação foi atualizada e estou pronto para clicar em Finalizar laboratório. Depois de clicar em Finalizar Laboratório, o projeto fornecido pelo Qwiklabs e todos os recursos desse projeto serão excluídos.
3:24
Eu posso fechar uma página de laboratório da Qwiklabs e minha nota será atualizada com a minha pontuação de laboratório no Coursera. É isso para este tutorial. Lembre-se de lançar o Coursera em uma janela anônima e use as credenciais fornecidas pelo Qwiklabs para fazer login no Console do GCP. Boa sorte com os laboratórios e aproveite o restante deste curso.


>>> Video 11  <<< 5 minutos.

Escolher quais produtos de big data e de aprendizado de máquina são a combinação certa para sua solução é uma habilidade essencial que você precisa aprender. Posteriormente neste módulo, você terá a oportunidade de examinar a arquitetura que os verdadeiros clientes do Google Cloud usam para poderem servir de inspiração. Vamos revisar as opções disponíveis para serviços de computação e armazenamento, para que você possa interpretar melhor esses casos de uso posteriormente. O serviço que pode ser o mais familiar para os recém-chegados é o Compute Engine, que permite a execução de máquinas virtuais sob demanda na nuvem. É a solução de Infraestrutura como Serviço ou IaaS do Google Cloud. Ele fornece flexibilidade máxima para pessoas que preferem gerenciar as próprias instâncias do servidor. GKE, o Google Kubernetes Engine é diferente. Onde o Compute Engine é sobre computadores individuais que executam código nativo, o GKE trata de clusters de máquinas que executam contêineres. Os contêineres têm o código empacotado com todas as suas dependências. Assim, o GKE permite que você execute aplicativos em contêiner em um ambiente na nuvem que o Google gerencia para você sob seu controle administrativo. A containerização é uma maneira de empacotar o código projetado para ser altamente portátil e usar os recursos de maneira muito eficiente. Como a maioria dos casos de uso envolve vários programas que precisam ser executados e se comunicar uns com os outros, você precisa de uma maneira de orquestrar os contêineres que executam esses programas separados. É o que o Kubernetes faz.
O Kubernetes é uma maneira de orquestrar o código que está sendo executado em contêineres. Todos os Kubernetes no GKE estão fora do escopo deste curso, estão ligados às nossas especializações de arquitetura de nuvem nos recursos do curso. O App Engine é a plataforma totalmente gerenciada pelo GCP, como um serviço ou uma estrutura de PaaS. Isso significa que é uma maneira de executar código na nuvem sem precisar se preocupar com infraestrutura. Você se concentra apenas no seu código e permite que o Google lide com todo o provisionamento e o gerenciamento de recursos. Você pode aprender muito mais sobre o Google App Engine na especialização Desenvolvendo aplicativos no Google Cloud Platform. O Cloud Functions é um ambiente de execução completamente sem servidor ou Funcations as a Service, FaaS. Ele executa seu código em resposta a eventos, independentemente de esses eventos ocorrerem uma vez por dia ou muitas vezes por segundo. O Google dimensiona os recursos conforme necessário, mas você paga apenas pelo serviço enquanto seu código é executado. Qual é a diferença entre o App Engine e o Cloud Functions? Normalmente, o App Engine é usado para aplicativos da Web de longa duração que podem ser escalados automaticamente para milhões, para bilhões de usuários. As Cloud Functions são usadas para códigos que são acionados por um evento, como um novo arquivo que atinge o armazenamento em nuvem. A maneira mais rápida de elevar e deslocar as cargas de trabalho de dados é provisionando uma VM e executando seu código. Você experimentará isso mais tarde quando executar tarefas do Spark ML no Cloud Dataproc, o que gerará instâncias do Compute Engine para seu cluster. A maioria dos aplicativos precisa de um banco de dados de algum tipo. Se você criou um aplicativo em nuvem, pode instalar e executar seu próprio banco de dados em uma máquina virtual no Compute Engine. Você simplesmente inicia a máquina virtual, instala seu mecanismo de banco de dados e configura-a exatamente como em um datacenter. Como alternativa, você pode usar os serviços de banco de dados e armazenamento totalmente gerenciados do Google. O que tudo isso - Bigtable, Cloud Storage, Cloud SQL, Spanner, Datastore - o que tudo isso tem em comum é que eles reduzem o trabalho necessário para armazenar diferentes tipos de dados. O GCP oferece bancos de dados relacionais e não relacionais e armazenamento mundial de objetos. Você aprenderá mais sobre isso mais adiante neste curso. O GCP também oferece serviços de big data e aprendizado de máquina totalmente gerenciados. Assim como com os serviços de armazenamento e banco de dados, você pode criar e implementar esses serviços por conta própria. Mas por que gerenciar a infraestrutura para computação e armazenamento, onde pode ser totalmente gerenciado pelo Google Cloud? Aqui está uma lista completa de big data e produtos ML organizados por onde você provavelmente os encontraria em uma carga de trabalho típica de processamento de dados. À esquerda, você verá a base para onde seus dados brutos estão armazenados. Se seus dados ainda não estiverem armazenados no GCP, você poderá utilizá-los usando as ferramentas que você vê a seguir. Depois que seus dados estiverem no GCP, você poderá analisá-los usando as ferramentas da terceira coluna e executar o aprendizado de máquina com as ferramentas da quarta coluna. A última coluna é como você pode exibir seus dados e insights de ML para seus usuários.


>>> Video 12  <<< 3 minutos.

Agora, é hora de explorar algumas soluções bacanas de Big Data e Machine Learning que foram criadas usando o Google Cloud. Em seguida, você terá a chance de encontrar um caso de uso e explorá-lo em sua própria atividade. A Keller Williams é uma empresa imobiliária dos EUA. Keller Williams usa AutoML Vision para reconhecer automaticamente características específicas de casas, como esta casa tem uma estante embutida. Isso ajuda os agentes a localizar as casas mais rapidamente e os compradores encontram casas que atendem às suas necessidades. Neil Dholakia, diretor de produto da Keller Williams, diz que ao treinar um modelo de aprendizado de máquina personalizado para reconhecer elementos comuns de mobiliário e arquitetura, os clientes podem pesquisar automaticamente fotos de imóveis específicos, como bancadas de granito ou estilos mais gerais como "Mostrar eu casas modernas ". Essa aplicação de aprendizado de máquina permite que os corretores de imóveis de Keller Williams caminhem rapidamente em torno de uma casa e gravem um vídeo e usem os recursos de detecção de objetos do AutoML Vision para localizar e marcar os principais aspectos da casa que os clientes podem pesquisar. Um grande benefício para a organização deles é que eles já tinham muitas imagens e vídeos já existentes de home walk-throughs. Eles simplesmente os inseriram no modelo AutoML Vision pré-criado e o personalizaram, tudo sem escrever uma linha de código. Você aprenderá mais sobre o AutoML Vision e praticará a criação de modelos de aprendizado de máquina com ele posteriormente neste curso. Ocado, o supermercado de supermercados on-line do Reino Unido, usava o aprendizado de máquina para encaminhar automaticamente os e-mails para o departamento que precisava para lidar com eles. Isso evita várias rodadas de leitura e triagem desses e-mails. Com o processo antigo, todos os e-mails foram para uma caixa de correio central na qual o e-mail foi lido e, em seguida, encaminhados para a pessoa ou departamento que poderia lidar com isso. Infelizmente, a caixa de correio central com alguém lendo todos os e-mails, isso não é escalável. Isso levou a longos atrasos e pouca experiência do usuário. Portanto, a Ocado usava o aprendizado de máquina, especificamente a capacidade de ler um e-mail, processar linguagem natural, descobrir o sentimento do cliente e de que mensagem era para que pudesse rotear de maneira imediata e automática. Um último caso de uso. Kewpie fabrica comida para bebé. Neste caso, a qualidade não é necessariamente uma questão de segurança, porque a comida em si é segura. Mas se a comida de bebê estiver descolorida, isso tende a deixar os pais muito preocupados. Então, Kewpie procurou o Google e nosso parceiro BrainPad para criar uma solução que aproveitasse o reconhecimento de imagens para detectar cubos de batata descoloridos ou de baixa qualidade. O algoritmo de aprendizado de máquina permitiu que libertassem as pessoas do cansativo trabalho de inspeção e se concentrassem em outro trabalho mais importante.


>>> Video 13  <<< 7 minutos.

Agora é sua vez. Uma das melhores maneiras de inspirar e direcionar sua equipe e seus projetos para a nuvem é mostrar aos seus participantes exemplos do seu setor, exemplos em que alguém já conseguiu criar uma solução. Para esta atividade, navegue até cloud.google.com/customers e role para baixo. Para Produtos e Soluções, filtre em análise de big data e também em aprendizado de máquina. Selecione um caso de uso de cliente que lhe interessa e responda a essas três perguntas. Número um, quais foram as barreiras ou desafios que o cliente enfrentou? Os desafios são importantes, você quer entender o que eles eram. Em segundo lugar, como esses desafios foram resolvidos com uma solução de nuvem? Quais produtos eles usaram? Três, qual foi o impacto nos negócios? Reserve um momento para concluir a atividade e, em seguida, retorne ao vídeo.
1:19
Para nosso exemplo, escolhemos o GO-JEK, porque eles usam uma solução de engenharia de dados que mapeia muito bem os tópicos que abordaremos como parte deste curso. A GO-JEK é uma empresa com sede na Indonésia que oferece passeios de motocicleta compartilhados, traz mercadorias e oferece uma ampla variedade de outros serviços para mais de dois milhões de famílias em 50 cidades da Indonésia. Seu App tem mais de 77 milhões de downloads, e eles estão conectados com mais de 150.000 comerciantes que vendem através de sua plataforma de entrega. Se você estiver interessado em dados de GIS, eles têm mais de um milhão de motoristas entregando mercadorias e dando passeios em 50 cidades e o GO-JEK obtém dados censurados de todos esses drivers. Assim, o GO-JEK gerencia mais de cinco terabytes por dia de dados para análise. O Chief Technology Officer, o CTO, Ajey Gore, dá esta estatística significativa. Por exemplo, fazemos ping de todos os nossos drivers a cada 10 segundos. O que significa, seis milhões de pings por minuto e oito bilhões de pings por dia, é o Gore quem está dizendo isso. Se você observar a escala e o número de nossas interações com os clientes também,

gerar cerca de quatro terabytes para cinco terabytes de dados todos os dias. Precisamos aproveitar esses dados para informar aos nossos motoristas onde a demanda dos clientes é mais forte e como chegar lá. Com o sucesso de seu serviço de passeio de motocicleta sob demanda, a GO-JEK enfrentou os desafios ao procurar escalar sua plataforma de big data existente. Quais desafios? A equipe de gerenciamento afirmou: "A maioria dos relatórios é produzida um dia depois, por isso não conseguimos identificar os problemas que estavam ocorrendo o mais rápido possível". O GO-JEK escolheu o Google Cloud Platform e migrou os pipelines de dados para o GCP, para que pudessem obter um alto desempenho com manutenção diária mínima. Sua equipe de engenharia de dados usa o Cloud Dataflow para streaming de processamento de dados e o Google Big Query para insights de negócios em tempo real. Seu fim para a arquitetura se parece com isso. Primeiro, eles ingerem dados de seus dispositivos móveis on-line e de IoT em veículos, como o rastreamento por GPS para entregas. Eles ingerem esses dados no Cloud PubSub. Em seguida, os dados são trazidos para o Cloud Dataflow para processamento e várias outras fontes de dados são usadas para enriquecer esses dados de evento. Por fim, depois que o Dataflow fez o processamento, o Dataflow transmite esses dados para o BigQuery e o BigQuery nesse caso é usado como um data warehouse para armazenar os dados. Qual é o impacto nos negócios? Então, aqui está um exemplo de um dos problemas que a equipe do GO-JEK resolveu. A questão era, como eles poderiam saber rapidamente quais locais tinham muitos drivers ou poucos drivers? Poucos drivers para atender a demanda por essa área. Para resolver esse problema, o que a equipe precisa fazer? Número um, eles precisavam verificar a demanda de reservas por cliente contra o fornecimento de drivers e fazer isso em tempo real. Em seguida, a equipe precisava identificar quem eram esses motoristas e notificá-los para redirecionar para áreas de maior demanda. Como eles conseguiram isso, foram construindo um pipeline de dados de evento de streaming usando o Cloud Dataflow. Os locais dos drivers seriam direcionados para o Pub / Sub a cada 30 segundos e esses locais entrariam no Dataflow para processamento. Os dados para o pipeline agrega os pings de fornecimento dos drivers em relação às solicitações de reservas e, em seguida, se conecta ao sistema de notificação do GO-JEK para alertar os drivers. Do ponto de vista tecnológico, o sistema precisa lidar com um alto throughput arbitrário de mensagens e escalar para cima e para baixo para processar. Cloud Dataflow automaticamente

drivers de alerta. Do ponto de vista tecnológico, o sistema precisa lidar com um alto throughput arbitrário de mensagens e escalar para cima e para baixo para processar. O Cloud Dataflow gerencia automaticamente vários funcionários, processando o pipeline para atender à demanda. A equipe do GO-JEK pode, então, visualizar e destacar áreas de incompatibilidade entre oferta e demanda para relatórios de gerenciamento, como você vê neste exemplo aqui. Os pontos verdes, os pequenos pontos verdes, representam os pilotos e os novos pedidos de reserva, e os pontos vermelhos, os pequenos pontos vermelhos, esses são os drivers, então você tem pilotos nos pontos verdes e os pilotos nos pequenos pontos vermelhos. Então você vê as áreas que o sistema identificou como um maior descompasso de oferta e demanda, essas áreas são destacadas em vermelho como a estação de trem que tem muitos pedidos de reserva, mas não drivers suficientes. A equipe agora pode monitorar ativamente e garantir que eles estejam enviando motoristas para as áreas com maior demanda, o que significa tempos de reserva mais rápidos para os passageiros e mais feiras para os motoristas.



>>> Video 14  <<< 6 minutos.

Até agora, você viu a infraestrutura, o software e os clientes que já usam o GCP. Mas o fator mais crítico para o sucesso de seus futuros projetos de Big Data e ML é sua própria equipe. As pessoas e os conjuntos de habilidades essenciais necessários farão ou quebrarão sua próxima inovação. Um erro comum que as empresas cometem é que elas contratam 10 cientistas aprendizes de máquinas de PhD e esperam que a mágica aconteça. Eu vejo muito isso com empresas que são novas na construção de ciência de dados e equipes de ML. Eles se concentram nos pesquisadores do ML e esquecem toda a ajuda e orientação que os pesquisadores do ML precisarão. A realidade, como minha colega Cassie observou em um post no blog, parece mais com isso. Você precisa de engenheiros de dados para construir os pipelines e obter dados limpos. Tomadores de decisão, para decidir o quão profundo você deseja investir em uma oportunidade baseada em dados enquanto pesa os benefícios para a organização. Analistas, para explorar os dados de insights e possíveis relacionamentos que poderiam ser úteis como recursos em um modelo de aprendizado de máquina. Os estatísticos, para ajudar a tornar suas decisões inspiradas em dados, tornam-se verdadeiras decisões baseadas em dados, com seu rigor adicional. Engenheiros de aprendizado de máquina aplicados, que têm experiência no mundo real, construindo modelos de aprendizado de máquina de produção com as mais recentes e melhores informações e pesquisas realizadas pelos pesquisadores. Cientistas de dados, que têm o domínio sobre análise, estatística e aprendizado de máquina. Gestores analíticos para liderar a equipa. Cientistas sociais e eticistas para garantir que o impacto quantitativo esteja presente em seu projeto e, é a coisa certa a fazer. Como escrevi em um post sobre este assunto, está vinculado abaixo, uma única pessoa pode ter uma combinação dessas funções, mas isso depende do tamanho da sua organização. O tamanho da sua equipe é um dos maiores motivadores para você contratar um determinado conjunto de habilidades, aprimorar sua habilidade interna ou combinar os dois. Você se lembra desses dados grandes desafios? Você consegue ver como papéis diferentes seriam mapeados para isso? Nos treinamentos do Google Cloud, minha equipe e eu pensamos sobre os diferentes tipos de equipes e funções de ciência de dados que usam o Google Cloud, para que possamos adaptar melhor nossos dados em cursos e laboratórios de ML. Um dos principais desafios que enfrentamos é como tipos diferentes de usuários se engajam com nossos produtos de grande volume de dados e AI do GCP. Nós estaremos usando algumas pessoas neste curso. Suas origens, objetivos e desafios podem ser semelhantes aos seus. Vamos encontrá-los agora e você os verá novamente mais tarde. Brittany e Theo lideram sua equipe de engenharia de dados no gerenciamento de seu cluster do Hadoop para os pipelines de dados e os trabalhos de computação da organização. Sua organização foi uma das primeiras a adotar o Hadoop para computação distribuída em 2007, e eles criaram trabalhos do Hadoop ao longo do tempo. O trabalho de Brittany e Theo é garantir ativamente que todos os trabalhos do Hadoop sejam executados e que o cluster seja bem mantido. Eles dizem: "Nosso CTO desafiou nossa equipe de engenharia de dados para encontrar maneiras de gastar menos com o gerenciamento de nosso cluster local. Agora, só queremos mostrar as opções que não exigem alterações de código em nossos 100+ Hadoop empregos." Brittany e Theo são engenheiros de dados que gerenciam a plataforma de dados de uma empresa e estão focados na redução da carga de manutenção. Jacob é um analista de dados que tem experiência em construir e consultar o banco de dados de transações e relatórios do MySQL de sua empresa. À medida que a empresa cresce, as tabelas de relatórios em seu RDBMS já começam a desacelerar. Os usuários estão relatando longos tempos de carregamento de consultas e painéis. Ele quer encontrar um caminho fácil para escalar os relatórios de dados de sua empresa e não ter que gerenciar outro sistema de dados, pois os dados precisam crescer. Jacob é um analista de dados que quer ser capaz de obter insights de dados e divulgá-los com o mínimo de atrito possível. Rebecca é engenheira de dados, cuja empresa é especializada em aproveitar dados da Internet of Things ou de dispositivos IoT. Ela diz: "Eu realmente quero projetar nossos pipelines de dados para o futuro. Para nós, isso significa muitos e muitos dados de fluxo de dados da nossa IoT 
Seu líder de equipe pediu a ela que elaborasse um plano para lidar com o esperado crescimento de 10X em volumes de dados de streaming este ano. Ela quer testar futuramente os pipelines de sua equipe, mas não quer gastar horas escalando manualmente hardware para cima e para baixo como fluxo de volume de mudanças.Além disso, sua equipe de partes interessadas de negócios quer insights de todos os dispositivos IoT no campo em seus painéis, com o mínimo de atraso.Vishal diz: "Eu armei minha equipe no aprendizado de máquina de valor pode adicionar e Eu tenho buy-in para construir um protótipo. Mas agora eu tenho que construir um protótipo. Quais são algumas das maneiras mais fáceis de ver se o aprendizado de máquina é viável para os meus dados? "Vishal é um engenheiro de aprendizado de máquina aplicado, que tem experiência em construir modelos de aprendizado de máquina em TensorFlow e Keras. Sua equipe está crescendo rapidamente e ele muitas vezes perguntado por sua liderança, para avaliar a viabilidade de aprendizado de máquina para uma ampla variedade de projetos.Ele não tem tempo para treinar e testar todas as idéias com modelos personalizados.Ele quer capacitar seus analistas de dados e equipes de engenharia de dados ensinando-lhes aprendizado de máquina. Essas pessoas parecem familiares para sua função e sua equipe? Em seguida, aprenderemos mais sobre as abordagens e soluções de big data e aprendizado de máquina do Google Cloud Platform, para que possamos abordar cada um desses desafios. .


>>> Video 15  <<< Introduction to machine learning

Agora que você viu como os sistemas de recomendação são usados ​​no Google e em outras empresas, é hora de analisar o cenário específico que usaremos no próximo laboratório. As principais peças de um sistema de recomendação são: dados, o modelo e a infraestrutura para treinar e fornecer recomendações aos usuários. Nosso conjunto de dados para este cenário será o aluguel de imóveis que queremos recomendar aos nossos usuários com base em suas preferências. Usaremos um modelo de aprendizado de máquina para fazer essas recomendações. Um princípio fundamental do aprendizado de máquina é permitir que o modelo aprenda por si mesmo qual é a relação entre os dados que você tem, como as preferências do usuário e os recursos de hospedagem, e os dados que não possui, como a classificação de um usuário em uma propriedade. ainda não vi. O que esse usuário classificaria essa propriedade se mostrássemos a ela? Você não estará escrevendo lógica personalizada. Lógica como, se a casa é uma casa de praia e a estação é verão e o usuário gosta de casas aconchegantes, então o número 22 é o que recomendamos. Você pode pensar nos problemas se usarmos essa lógica em vez de deixar o modelo descobrir isso? E se houvesse muitos tipos diferentes de casas de praia, com todos os tipos de características diferentes, como o número de quartos, locais, comodidades, você obtém a imagem. E se você tiver diferentes tipos de usuários. Alguns usuários gostariam de ir a casas de praia no verão e outros gostariam da estação do ombro. A lógica de codificação rígida para todos esses recursos e diferentes tipos de segmentos de usuários não é escalonável e pressupõe que sempre sabemos a resposta certa para cada cenário. Vamos ilustrar esse ponto com um exemplo. Pegue a Pesquisa do Google. Digamos que você vá ao Google e pesquise por "gigantes". O que devemos mostrar a você como os resultados para torná-lo mais relevante para você? Se você está na Califórnia, devemos mostrar resultados para o time de beisebol de San Francisco e os jogos locais nas proximidades? E se você estiver em Nova York? Devemos adaptar os resultados para mostrar o time de futebol do New York Giants, e devemos escrever isso como regra? Bem, até alguns anos atrás, era exatamente assim que a pesquisa do Google funcionava. Havia várias regras que faziam parte da base de código do mecanismo de pesquisa para decidir qual time esportivo exibir um usuário quando ele pesquisar "gigantes". A consulta é "gigantes" e o usuário está na área da baía, mostre os resultados sobre os gigantes de São Francisco. Se o usuário estiver na área de Nova York, mostre os resultados sobre os New York Giants. Se eles estiverem em qualquer outro lugar, mostre a eles resultados sobre pessoas altas. Imagine quantas instruções if-then ou case seriam e quão difícil seria manter, e isso é apenas para uma consulta. Multiplique isso pela grande variedade de consultas que as pessoas fazem, onde fazê-los, em que dispositivo eles estão e que tipos de interesses têm pessoas diferentes, e você pode imaginar como uma base de código se tornaria complexa. O código base começa a ficar pesado porque as regras codificadas manualmente são realmente difíceis de manter. Então é aí que o aprendizado de máquina entra em cena. O aprendizado de máquina é muito melhor porque não exige regras codificadas. Tudo é automatizado. Aprendendo com os dados de maneira automatizada, é isso que a aprendizagem de máquina é. Nosso conjunto de dados, neste caso, é que sabemos historicamente quando as pessoas pesquisaram "gigantes" e mostramos a eles um monte de links, sobre quais desses links as pessoas clicaram? Como sabemos disso, temos um conjunto de dados de um termo de consulta e os links de que as pessoas gostaram. Por que não podemos treinar um modelo de aprendizado de máquina para basicamente fornecer uma classificação desses links? Isso é exatamente o que o próprio Google fez internamente com um modelo de aprendizado profundo chamado Rank Brain. Depois de lançá-lo, a qualidade de nossos resultados de classificação de pesquisa melhorou drasticamente, com o sinal da Rank Brain se tornando um dos três principais para influenciar a classificação dos resultados de links diferentes. Se você estiver interessado, eu vou fornecer um link onde você pode ler mais sobre isso. Vamos revisitar o aprendizado de máquina com maior profundidade em cada um dos módulos deste curso. Mas, por enquanto, lembre-se de que o aprendizado de máquina é essa ideia que queremos ensinar ao computador usando exemplos, não com regras. Qualquer aplicativo de negócios em que você tenha essas longas declarações de switch ou case ou lógica if-then codificada manualmente e tenha um histórico de bons dados rotulados, ou seja, dados para os quais você sabe uma boa resposta ou uma resposta ruim, um histórico de boas dados, qualquer aplicativo desse tipo é uma possível aplicação para aprendizado de máquina.


>>> Video 16  <<< Challenge: ML for recommending housing rentals.

Então, como as recomendações de habitação funcionariam em nosso sistema? Primeiro, precisamos ingerir as avaliações de todas as casas que já foram feitas por nossos usuários quando mostramos a eles casas específicas. Então, temos que ir para um inventário de aluguéis e ingerir as classificações para as casas. Essas classificações podem vir de classificações explícitas. Talvez tenhamos mostrado ao usuário a casa no passado e eles clicaram quatro estrelas depois de ver os detalhes da casa, ou as classificações vieram de avaliações implícitas. Talvez eles tenham passado muito tempo olhando o site correspondente a essa propriedade. Então, vamos treinar um modelo de aprendizado de máquina para prever a classificação de cada casa que temos atualmente em nosso banco de dados. Em seguida, escolheremos as cinco principais casas classificadas que eles ainda não viram. As pessoas que não estão cientes dos mecanismos de recomendação de alguma forma pensam em um carro aparecendo em seu feed do Facebook porque leram um artigo de revisão de automóveis. Eles dizem: "Ah, eu estava lendo este artigo de revisão e depois ficava vendo carros no meu feed do Facebook". Esse não é o caso. A leitura do artigo fez com que a classificação de todos os carros aumentasse e as classificações de outros itens que não são carros permanecessem relativamente as mesmas. Isso fez com que o carro mais bem classificado chegasse ao top cinco. Portanto, sempre houve uma classificação para o carro. O carro estava sempre lá, mas sua classificação era simplesmente menor antes de ler o artigo em questão. Então, como poderíamos prever a classificação de um usuário de uma casa, particularmente, se eles não a tivessem visto antes? O modelo é baseado em duas coisas. É baseado em suas outras avaliações, o que você avaliou outras casas, e classificação de outras pessoas desta casa particular. Um modelo particularmente simples poderia ser olhar para todos os usuários que avaliaram esta casa em particular e encontrar os três usuários em sua lista que são mais parecidos com você, talvez, eles moram em seu país, talvez tenham a mesma idade, talvez na mesma faculdade, encontre os três usuários dessa lista que são mais parecidos com você. Então, a previsão é a média dessas três classificações de usuários. Este não é um ótimo modelo, é claro. É muito fácil de jogar. Pense no que acontece se três pessoas se juntam e avaliam uma casa, uma casa que ninguém mais se importa. Portanto, existem apenas três classificações para esta casa. Então os três usuários mais próximos serão essas três pessoas. Então, para casas esparsas, é muito, muito, muito fácil de jogar. Mas essa ideia de usar classificações de usuário de uma determinada casa e usuários como você ajuda a transmitir a premissa básica de como os modelos de recomendação funcionam. Então, onde está a máquina aprendendo aqui? Onde está o aprendizado? O modelo seria, 
tem que descobrir como encontrar os usuários mais parecidos com você. Quantos usuários considerar. Três usuários, cinco usuários, sete usuários, quantos? E como ponderar os diferentes fatores, como a popularidade geral dos itens que você tem em comum e assim por diante. Isso pode ser feito ver quais parâmetros ajudam a prever se você intencionalmente reteve melhor as classificações. Então, podemos ter milhares de itens e apenas 2-3 comentários por item. As chances são de que os revisores não tenham nada em comum com o usuário para o qual queremos uma classificação. Então, como essa matriz de classificação é extremamente esparsa, precisamos agrupar itens e usuários juntos. Para colocar isso de uma forma mais intuitiva, imagine que todos os seus amigos dirigem SUVs, Sport Utility Vehicles e você lê um artigo sobre a Porsche. O carro que aparece em seu feed pode ser um SUV da Porsche, mesmo que o artigo esteja em carros da Porsche e que todos os seus amigos dirigam SUVs da Toyota. O modelo de aprendizado de máquina imputa uma classificação para um SUV da Porsche que é bastante alto, mesmo que nenhum de seus amigos o avalie. Então, o modelo de aprendizado de máquina está essencialmente perguntando "Quem é esse usuário?" Em segundo lugar, esta é subjetivamente uma casa que as pessoas tendem a avaliar altamente? A classificação prevista é uma combinação desses dois fatores. Tudo considerado, a classificação de uma casa para um usuário em particular será a média das avaliações de usuários como este usuário, mas é calibrada com a qualidade do item em si. Agora que entendemos o problema e a abordagem, precisamos abordar a última questão, a questão da infraestrutura. Com que frequência e onde você calculará as avaliações previstas e, depois de tê-las, depois de computar essas classificações, onde as armazenaria? O que você acha? Com que frequência e onde você calculará as classificações previstas? Não é como se as recomendações de aluguel tivessem que ser atualizadas sempre que alguns usuários avaliassem uma casa. Não precisamos atualizar as recomendações de aluguel toda vez que uma nova classificação aparecer em nosso sistema. É provavelmente suficiente atualizarmos as casas recomendadas para os usuários uma vez por dia, talvez até uma vez por semana. Em outras palavras, isso não precisa ser, 
transmissão. Pode ser em lote. Por outro lado, provavelmente teremos milhares de casas e milhões de usuários. Então, é melhor que calculemos a classificação que cada usuário dará a cada casa, fazemos essa computação de maneira escalável. Nós não queremos fazê-lo em uma única máquina, queremos fazê-lo de uma maneira tolerante a falhas que pode escalar para grandes conjuntos de dados. Portanto, uma solução típica para computação que tem que acontecer em grandes conjuntos de dados de uma maneira tolerante a falhas é fazê-lo em uma plataforma de big data como o Apache Hadoop. Finalmente, onde você irá armazenar as avaliações computadas? Por que você iria querer armazená-los? Bem, nós provavelmente queremos alimentar um aplicativo da web com essas recomendações e não queremos computar recomendações quando o usuário lê uma página da web. Queremos precomputar essas recomendações, como dissemos, é um trabalho em lote. Então, uma vez por dia, uma vez por semana, pré-calculamos isso. Então, quando o usuário fizer logon, queremos mostrar a esse usuário as recomendações que pré-computamos especificamente para elas. Então, precisamos de uma maneira transacional de armazenar as previsões. Por que transacional? Para que, enquanto o usuário estiver lendo essas previsões, também possamos atualizar a tabela de previsões. Supondo que haja cinco previsões para cada usuário e que tenhamos um milhão de usuários, essa é uma tabela de apenas 5 milhões de linhas. É pequeno o suficiente e compacto o suficiente para que uma solução típica para isso seria armazenar os dados em um sistema de gerenciamento de banco de dados relacional, um RDBMS como o meu SQL.



>>> Video 17 <<< Approach: Move from on-premise to Google Cloud Platform

Por isso, decidimos usar uma plataforma de big data como o Hadoop e depois o RDBMS, como o MySQL, para resolver o problema de recomendações habitacionais. Estas são ambas tecnologias de código aberto. Você provavelmente tem clusters do Hadoop e banco de dados MySQL em execução no local. Então, vamos supor que sua equipe já tenha esse sistema de recomendação trabalhando no local e veja como migrá-lo do local para o Google Cloud Platform. Claro que você quer fazer isso apenas se a migração puder agregar valor. Então, vamos ver isso também. Para um modelo de recomendação de alojamento, digamos que nossa equipe de ciência de dados já tenha um modelo local de trabalho usando o trabalho SparkML em seu cluster do Hadoop. Eles estão interessados ​​na escala e na flexibilidade do GCP, e querem fazer uma migração semelhante para seus trabalhos SparkML existentes no local e desejam fazer isso como um projeto piloto. Conforme explicado na lição anterior, podemos nos livrar da classificação prevista uma vez por dia. Não precisamos computar recomendações em tempo real. Portanto, o processamento em lote do Hadoop é suficiente. Aqui, usaremos o SparkML, mas em vez de fazê-lo no local, executaremos o trabalho de aprendizado de máquina no Cloud Dataproc e armazenaremos as classificações em um RDBMS no Cloud SQL, porque esse é um conjunto de dados relativamente pequeno de cinco recomendações para cada usuário. Então é isso que você vai praticar no seu primeiro laboratório. Aliás, é por isso que a resposta inteligente no Gmail é tão incrível. Ao contrário das nossas recomendações de invólucro, as recomendações das respostas do Spark devem ser feitas em tempo real, quando um novo e-mail aparece na sua caixa de correio. Escusado será dizer que o Gmail está fazendo algo muito mais sofisticado do que o que estamos falando aqui. Veja como decidimos usar o Cloud SQL entre os outros produtos de big data disponíveis para armazenar nossas classificações. Esta é uma boa referência a seguir com base no seu padrão de acesso ao armazenamento. Abordaremos as soluções em seus outros cenários neste curso, mas, resumidamente, use o Cloud Storage como um sistema de arquivos global. Use o Cloud SQL como um RDBMS como um sistema de gerenciamento de banco de dados relacional para dados relacionais transacionais acessados ​​por meio do SQL. Usar Datastore como um banco de dados orientado a objetos No-SQL transacional. Use o Bigtable para dados somente de anexação No-SQL de alto rendimento. Portanto, não transações, apenas dados anexados. Portanto, um caso de uso típico do Bigtable é, por exemplo, dados de sensores para dispositivos conectados. Use o BigQuery como um data warehouse de SQL para alimentar todas as suas necessidades de análise. Então, aqui nós queríamos um banco de dados transacional e esperamos ter volumes de dados nos gigabytes ou menos e, portanto, o Cloud SQL. Se você for um aprendiz mais visual, veja outro bom mapa para visualizar onde armazenar seus dados no GCP. Se seus dados não estiverem estruturados, como imagens ou áudio, use o Cloud Storage. Se seus dados estiverem estruturados e você precisar de transações, use o Cloud SQL ou o Cloud Datastore, dependendo de seu padrão de acesso ser SQL ou No-SQL e por No-SQL, queremos dizer um par de valor-chave. Em outras palavras, você tentará pesquisar dados com base em uma única chave, usar o Datastore se encontrar dados usando o SQL Cloud SQL. O Cloud SQL geralmente se destaca em alguns gigabytes. Portanto, se você quiser um banco de dados transacional escalonável horizontalmente para poder lidar com dados maiores do que alguns gigabytes ou se precisar de vários bancos de dados, para que eles sejam distribuídos globalmente, use o Cloud Spanner. Então, outra maneira de dizer isso como se um banco de dados fosse suficiente, use o Cloud SQL. Se você precisar de vários bancos de dados, seja porque tem muitos dados ou porque seu aplicativo precisa ser transacional em diferentes continentes, use o Cloud Spanner. Se seus dados estão estruturados e você quer o Analytics, considere o Bigtable ou o BigQuery. Use o Bigtable se você precisar de aplicativos de alto rendimento em tempo real. Use o BigQuery se você quiser o Google Analytics em conjuntos de dados em escala de petabytes. Para nosso caso de uso de recomendação de alojamento, queremos armazenar nossas classificações e previsões em algum lugar. Este é um conjunto de dados estruturados de classificações e residências de usuários. Ele é construído para uma carga de trabalho transacional, grava e lê e um banco de dados é suficiente para um conjunto de dados pequeno e, portanto, escolhemos o Cloud SQL. Então, qual é o Cloud SQL? É um banco de dados relacional hospedado e gerenciado pelo Google na nuvem. O Cloud SQL suporta dois bancos de dados de código aberto: MySQL e Postgres e outras soluções de banco de dados. No nosso caso, estaremos usando o MySQL. Uma das vantagens do Cloud SQL é que é familiar. O Cloud SQL suporta a maioria das instruções e funções do MySQL, mesmo procedimentos armazenados, disparadores e visualizações. Traz os benefícios da economia da nuvem na forma de preços flexíveis. Você pode pagar pelo que usa. O que queremos dizer com isso? O GCP gerencia a Instância do MySQL para você. Isso significa coisas como backup e replicação, está na nuvem para que você possa se conectar a ele de qualquer lugar. Você pode atribuir um endereço IP estático e usar bibliotecas típicas de conector SQL. Porque está por trás do firewall do Google, é rápido. Você pode colocar sua Instância do Cloud SQL na mesma região que seus aplicativos do Google App Engine ou do Compute Engine e obter uma grande largura de banda. Além disso, você recebe o Google Security. O Cloud SQL reside em datacenters seguros do Google. Então, isso abrange onde estaremos armazenando as recomendações. Mas como vamos calcular essas recomendações em primeiro lugar? Onde estaríamos fazendo o cálculo? Vamos rever onde os cálculos de big data foram feitos historicamente e até hoje. Antes de 2006, big data significava grandes bancos de dados. O design do banco de dados veio de uma época em que o armazenamento era relativamente barato e o processamento era caro. Portanto, fazia sentido copiar os dados de seu local de armazenamento para o processador para executar o processamento de dados e, em seguida, o resultado seria copiado de volta para o armazenamento. Por volta de 2006, o processamento distribuído de big data tornou-se prático com o Hadoop. A ideia por trás do Hadoop é criar um cluster de computadores e aproveitar o processamento distribuído, o HDFS. O Hadoop Distributed File System armazenava os dados nas máquinas do cluster e o MapReduce fornecia o processamento distribuído desses dados. Todo um ecossistema de software relacionado ao Hadoop cresceu em torno do Hadoop, incluindo Hive, Pig e Spark. Por volta de 2010, foi lançado o BigQuery, que foi o primeiro de muitos serviços de big data desenvolvidos pelo Google. Por volta de 2015, o Google lançou o Cloud Dataproc, que fornece um serviço gerenciado para a criação de clusters Hadoop e Spark e o gerenciamento de cargas de trabalho de processamento de dados. A outra parte do nosso sistema é um software que roda no Hadoop. O software, neste caso, é treinar um modelo de aprendizado de máquina para criar recomendações de moradia. Neste caso, usamos o SparkML, que faz parte do ApacheSpark. O ApacheSpark é um projeto de software de código aberto que fornece um mecanismo de análise de alto desempenho para processar dados em lote e streaming. O Spark pode ser até 100 vezes mais rápido que os trabalhos equivalentes do Hadoop, pois aproveita o processamento na memória. O Spark também fornece algumas abstrações para lidar com dados, incluindo os chamados RDDs ou Conjuntos de Dados e DataFrames Distribuídos Resilientes. Nós estaremos usando nosso trabalho Spark para as recomendações de aluguel de habitação. Usaremos nosso trabalho do Spark para as recomendações de aluguel de imóveis, mas faremos o cálculo no Cloud Dataproc.


>>> Video 18 <<< Demo: From zero to an Apache Spark job in 10 minutes or less.

Um dos casos de uso mais comuns é executar seus trabalhos do Apache Spark em seu cluster do Hadoop. Agora, vamos fazer uma demonstração da criação de um novo cluster e da execução de um trabalho do Spark usando o Cloud Dataproc, e nosso objetivo será fazê-lo em 10 minutos ou menos. Se você já viu um cluster Hadoop local sozinho, isso parece ser uma grande vitória para o tempo e a eficiência da configuração da infraestrutura. Vamos começar. Tudo certo. Aqui estamos no console do Google Cloud Platform. Este é o Cloud Dataproc. Como você chega aqui é se você abrir o menu de navegação, eu tenho vários favoritos que eu fixei aqui. Mas vou mostrar-lhe como, se você rolar para baixo nos Produtos e Serviços para Big Data, você verá muitos dos produtos que mencionamos até agora. O Dataproc está bem aqui. Se você clicar nesse alfinete, isso o levará até aqui, o que o salvará, o que é um bom truque de eficiência. Passando o mouse sobre o serviço do Cloud Dataproc, você verá que pode clicar em Clusters, Jobs ou Workflows. Mas se você clicar no logotipo, você será direcionado para a página Clusters. Então aqui nós temos isso. Está em branco, em branco ardósia. Portanto, a primeira coisa que queremos fazer é criar e provisionar um cluster do Apache Hadoop no Dataproc. Então, os detalhes da configuração, como você poderia esperar, estão aqui. Nós vamos dar o cluster um ótimo nome. Nós vamos estimar os dígitos do Pi de forma distribuída, então vamos chamar de cluster Pi. Aqui você pode escolher o tipo de disponibilidade que é essencialmente a razão de quantos nós mestres você deseja em seus n nós de trabalho, então vamos mantê-lo padrão. Se você quiser ajustar o hardware que é seu computador, suas CPUs, você pode fazer isso aqui até o momento da gravação, tendo 160 CPUs virtuais e memória ultra. Nós vamos manter apenas o padrão aqui e 15 gigabytes de memória, 500 gigabytes de disco persistente certeza, estas são todas as configurações. Então, se você está executando um projeto piloto para uma comparação de maçãs com maçãs, apenas mapeie o seu mesma infra-estrutura usando comida para seus clusters, e o número de nós que vamos manter, apenas os dois estão lá, e nós vamos em frente e clicamos em criar. Enquanto esse cluster está girando nos bastidores, vamos em frente e basta preencher um trabalho para começar. Então eu já fiz um trabalho anteriormente e você pode dizer que desde que eu não tenho um cluster antes disso eu realmente rejeitei esse cluster. Então, vamos criar um novo trabalho. Então, clique em Enviar trabalho e mantenha o ID do trabalho, ou apenas chamaremos esses dígitos Pi estimados. A região global está bem. É bom que tenhamos o nome do cluster mesmo que esteja girando e ele ainda tenha aqui. Você pode enviar um trabalho e enfileirá-lo para ser executado, os tipos de trabalho e não um trabalho do Hadoop é um trabalho do Spark. Você pode ver os diferentes tipos de trabalho que você pode selecionar aqui e agora definimos a classe Java. Este é o SparkPi - associei o exemplo, mas você pode realmente ver que há vários outros exemplos do Spark que pode ser executado e, basicamente, apenas gera uma tonelada de pares de coordenadas xy em um círculo unitário ou um círculo com um diâmetro de um dentro de um quadrado, que é o método de Monte Carlo para estimar o número de dígitos em Pi. Se você estiver interessado, eu ligarei isso lá, mas é essencialmente que estamos rodando isso e usando os nós de trabalho do Hadoop para processar isso em paralelo massivo e o exemplo real onde o arquivo jar é, eu vou apenas colar isso em lá do exemplo, e vamos gerar mil desses pares xy. Isso é apenas um argumento pelo qual estamos passando. Isso é específico para o trabalho que estamos executando. Nem sempre coloque um mil para os argumentos, a menos que o seu programa exija, e nós iremos adiante e o enviaremos. Agora, vai rodar, vamos ver se o cluster está pronto para uso. Sim, temos a marca de seleção verde aqui. Se quisermos, você pode clicar no cluster e obter um acompanhamento muito bom das métricas que você vê aqui. De dentro, você pode ver os trabalhos em execução. Nesse caso, nos últimos 30 segundos, tivemos esse trabalho de estimativa de dígitos Pi. Vamos em frente e monitorar esse trabalho especificamente. Naturalmente, você pode ter várias tarefas em execução em um cluster, vários clusters executando várias tarefas e boom, obtendo a saída do nosso trabalho. Nós vemos que começou aqui, e se nós rolarmos para baixo o suficiente, isso deve nos dar uma saída de log, boom. Aqui vamos nós. Onde nós realmente submetemos o aplicativo, e obtemos o resultado que é a estimativa do Pi aqui, e então o trabalho está completo. A faísca foi parada. Então, agora, o que você pode fazer é ter uma execução de trabalho bem-sucedida, levar 36 segundos mais o tempo necessário para acelerar o cluster, pois a potência real é, você pode ir em frente e desativar esse cluster agora. Agora, se você quisesse ter um cluster ao vivo longo porque você tinha dados persistentes sobre eles, com certeza absoluta. Falaremos um pouco mais sobre a separação de computação e armazenamento para tentar sair do uso de discos persistentes em clusters e usar algo como o Google Cloud Storage. Mas se não precisarmos mais desse cluster, não estamos tentando manter nenhum dado, já executamos nosso cálculo. É tão simples quanto clicar no ícone de exclusão e desativar esse cluster, não se preocupe em gerenciar a infraestrutura. Você tem o que precisava daquele trabalho em particular, ou, se o seu trabalho terminar, você quer enviar mais trabalhos ou reexecutar o mesmo trabalho aqui. Use quantos clusters, seja qual for o tamanho dos clusters, edite os recursos na hora. A grande conclusão é que os clusters são agora o que chamamos de recursos fungíveis, eles são efêmeros. Você os usa quando precisa deles para enviar seus trabalhos, se precisar deles por mais tempo ou se precisar deles mais rapidamente, se precisar de mais clusters ou menos clusters, poderá gerenciar essa infraestrutura automaticamente aqui por meio do Cloud Dataproc. Isso é essencialmente o que é. Então, meu desafio para você está dentro do seu laboratório um pouco mais tarde quando você está criando o mecanismo de recomendação usando o Cloud SQL e o Cloud Dataproc, experimente o Cloud Dataproc, observe os registros de cada um dos diferentes jobs em execução e você está executando cargas de trabalho no local dentro do Hadoop, veja se você pode experimentar alguns deles executando o Cloud Dataproc e ver qual deles é mais eficiente.




>>> Video 19 <<< Approach: Move from on-premise to Google Cloud Platform.

Dissemos que falaríamos sobre os desafios comuns do Big Data e como eles são abordados. Um dos desafios mais comuns para o gerenciamento de clusters Hadoop no local é garantir que eles sejam utilizados e ajustados de maneira eficiente para todas as cargas de trabalho que os usuários lançam neles. Então vamos checar com nossa equipe e ver quais desafios eles estão enfrentando executando o trabalho Spark ML no cluster local. Nossa equipe tem um data center local executando o Hadoop representado pela caixa azul aqui. Em qualquer semana, eles gerenciam quatro tarefas diferentes para a organização. Nosso trabalho de recomendação do Spark ML é um dos quatro. Os dados são mantidos no armazenamento tradicional baseado em disco HDFS. Então, aqui está um cenário de exemplo que pode parecer familiar para você. A equipe lança o job número 1, digamos, para argumentar, que é um job de big data pipeline que consome 50% dos recursos de cluster disponíveis no local. Em seguida, a equipe tem um pedido especial de marketing para executar seu trabalho de previsão para uma próxima campanha ainda hoje. Esse poderia ser o job número 2, e aumentará e consumirá os outros 50% dos recursos de cluster disponíveis. Você provavelmente verá o problema agora. Se o nosso trabalho Spark ML fosse o número 3 ou o número 4, ele ficaria sem recursos porque a capacidade de computação dos clusters está sob provisionamento para as demandas dos trabalhos do Hadoop da organização. Ou uma alternativa dispendiosa, é onde apenas uma tarefa está em execução e usa 50% dos recursos do cluster, e os outros 50% das máquinas são alimentados e disponíveis, mas não são necessários. O problema aqui está na natureza estática da capacidade do cluster no local. A equipe precisa de uma maneira melhor de otimizar o uso dos recursos do cluster sem a dor de cabeça de ajustar continuamente, adicionar e remover os próprios servidores. É aqui que o GCP pode ajudar. Agora você pode pensar em clusters como recursos flexíveis. Com o Cloud Dataproc, o produto gerenciado do Hadoop do GCP, você pode gerar tantos ou poucos recursos de cluster quanto o trabalho precisa na nuvem. Observe como eu disse recursos de cluster. Você os usa quando precisa executar trabalhos e desativá-los quando eles não são mais necessários. Então, nesse cenário, nós podem ter os jobs 1 e 2, executados em seu cluster personalizado número 1 e os jobs número 3 e 4 podem ser executados em seus próprios clusters também. Seus trabalhos recebem os recursos de que precisam. Você pode desligar os clusters quando eles não estiverem em uso. Os clusters se tornam recursos fungíveis. Você pode até mesmo automatizar quando o cluster desligar, para que você não pague por recursos que não está usando. Você pode definir acionadores de desligamento com base no tempo que um cluster permaneceu inativo ou em um registro de data e hora específico ou em segundos para aguardar ou quando enviar um trabalho, executar esse trabalho e encerrar. Mas e se as necessidades de um emprego mudarem e, em alguns dias, precisarmos de um cluster maior e, em alguns dias, precisarmos de um cluster menor? O escalonamento automático do Cloud Dataproc fornece capacidade flexível para ajudá-lo a atender a essa necessidade. Ele toma a decisão de dimensionamento observando as métricas do YARN do Hadoop. Você pode usar o escalonamento automático desde que, ao encerrar o nó do cluster, ele não remova nenhum dado. Portanto, você não pode armazenar os dados no cluster, mas é por isso que armazenamos nossos dados no Cloud Storage, no Bigtable ou no BigQuery. Armazenamos nossos dados no cluster. Portanto, o escalonamento automático funciona desde que você não armazene seus dados no HDFS. Além do escalonamento automático, outra vantagem de executar clusters do Hadoop no GCP é que você pode incorporar máquinas virtuais preemptivas à sua arquitetura de cluster. As VMs preemptivas são instâncias de computação altamente acessíveis e de curta duração que são adequadas para tarefas em lote e cargas de trabalho tolerantes a falhas. Por que tolerante a falhas? Como máquinas preemptivas, elas oferecem os mesmos tipos de máquinas e opções que as instâncias de computação regulares, mas elas duram apenas até 24 horas e podem ser removidas sempre que alguém aparecer e oferecer novas necessidades de computação para elas. Portanto, se seus aplicativos forem tolerantes a falhas e os aplicativos do Hadoop estiverem, as instâncias preemptivas poderão reduzir significativamente os custos com o Compute Engine. Quão significativo? VMs preemptivas são até 80% mais baratas que as instâncias regulares. O preço é fixo, você recebe um desconto de 80%. Assim, você sempre obtém previsibilidade financeira e de baixo custo sem correr o risco de apostar em preços de mercado variáveis. Mas, assim como o escalonamento automático, as VMs preemptivas funcionam quando sua carga de trabalho pode funcionar sem que os dados sejam armazenados no cluster. Você deseja armazenar esses dados no cluster e é isso que veremos a seguir.



>>> Video 20 <<< Move storage off-cluster with Google Cloud Storage.

Como você deve se lembrar da demonstração anteriormente, separar computação e armazenamento permite uma escala econômica para cargas de trabalho. Com a noção de desativar os clusters, você pode estar preocupado com todos os dados que estão atualmente armazenados no disco HDFS. Você pode estar usando o HDFS no local. O que acontece com a nova arquitetura do Google Cloud Platform? Seus dados não estão armazenados no cluster, seus dados agora estão armazenados fora do cluster nos intervalos do Google Cloud Storage. Realmente, isso não tornará as coisas lentas para alcançar uma rede toda vez que o cluster precisar de alguns dados? Lembre-se de que, anteriormente, mencionamos a largura de banda do data center do Google entre computação e armazenamento. Nós conversamos sobre uma largura de banda bipectional de petabit. O que isso significa é que, se você desenhar uma linha em algum ponto da rede, a largura de banda de divisão é a taxa de comunicação na qual os servidores de um lado da linha podem se comunicar com os servidores do outro lado. Com largura de banda de divisão suficiente, qualquer servidor pode se comunicar com qualquer outro servidor em velocidades de rede completas. Com a largura de banda bipectional do petabit, a comunicação é tão rápida que não faz mais sentido transferir os arquivos e armazená-los localmente. Em vez disso, faz sentido usar os dados de onde eles estão armazenados. Então aqui está o plano. Usaremos o HDFS, mas usaremos o HDFS apenas no cluster para armazenamento de trabalho, armazenamento durante o processamento, mas armazenaremos todos os dados reais de entrada e saída no armazenamento do Google Cloud. Como os dados de entrada e saída estão fora do cluster, o cluster pode ser criado para um único trabalho ou tipo de carga de trabalho e pode ser desligado quando não estiver em uso. Alterar o código que funciona no local para que funcione com os dados no armazenamento em nuvem é fácil. Basta substituir o HDFS em seu Spark ou Big Cloud por GS. Então pegue as URLs do HDFS, HDFS: // e substitua por GS: // Isso fará com que o Spark de grande trabalho leia ou grave no armazenamento em nuvem, é isso. Há também uma base H Como você deve se lembrar da análise anterior, a computação de separação e o suporte permitem uma escala econômica para as cargas de trabalho. Com uma observação de clusters, você pode estar preocupado com todos os dados que estão atualmente armazenados no disco HDFS. Você pode estar usando o HDFS no local. O que acontece com uma nova arquitetura do Google Cloud Platform? Os dados não estão armazenados no cluster, as suas informações são armazenadas no Google Cloud Storage. Really, that not tornará as coisas lentas para ter uma rede inteira que o cluster precisa de alguns dados? Lembre-se de que, anteriormente, mencionar a largura de banda do centro de dados do Google entre linguagem e armazenamento. Nós conversamos sobre uma largura de banda bipectional de petabit. O que é que é, se você pode desenhar uma linha em algum ponto da rede, a largura de banda de divisão é uma taxa de comunicação e os servidores de um mesmo grupo podem se comunicar com os servidores do outro lado. Com uma banda de escuta suficiente, o servidor está ligado a um espaço em rede. Com uma largura de banda bipectional do petabit, a comunicação é tão rápida que não faz mais sentido que os arquivos e armazená-los localmente. Em vez disso, faz uso dos dados de onde eles estão armazenados. Então aqui está o plano. Usar o HDFS, mas usar o HDFS apenas no cluster de armazenamento, durante o processo, mas armazenar todos os dados de entrada e saída no armazenamento do Google Cloud. Como os dados de entrada e saída estão fora do cluster, o cluster pode ser criado para um serviço único ou tipo de carga de trabalho e pode ser usado quando não estiver em uso. A modificação do código não funciona localmente para funcionar com os dados armazenados. Basta substituir o HDFS no seu Spark ou Big Cloud por GS. Então pegue as URLs do HDFS, HDFS: // e substitua por GS: // Isso faz com que o faísca seja de grande importância. Há também uma base H


>>> Video 21 <<< 